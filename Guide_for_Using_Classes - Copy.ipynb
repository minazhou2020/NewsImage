{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cba1de",
   "metadata": {},
   "source": [
    "# Sample Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f2892",
   "metadata": {},
   "source": [
    "**This is a sample showing how to use following five classes**\\\n",
    "\\\n",
    "Data_Preprocessing\\\n",
    "URL_Matching\\\n",
    "Face_Name_Matching\\\n",
    "Model_Ensembling\\\n",
    "Image_Caption.py\\\n",
    "\\\n",
    "\\\n",
    "Requirements:\n",
    "Python 3.7\\\n",
    "\\\n",
    "Libraries:\n",
    "tensorflow\\\n",
    "pytorch\\\n",
    "py-googletrans\\\n",
    "NLTK\\\n",
    "gensim\\\n",
    "cv2\\\n",
    "icrawler\\\n",
    "DeepFace\\\n",
    "scipy\\\n",
    "matplotlib\\\n",
    "\\\n",
    "What you can do with this class:\n",
    "\n",
    "#### Data Preprocessing\n",
    "- combine the first two batches of files for training usage\n",
    "- the third batch used for validation\n",
    "- crawl training, valation, and test images from given URLs \n",
    "- extract features: image id, image url, article_title\n",
    "- tranlsate article title into English using Google Translate API (https://github.com/ssut/py-googletrans)\n",
    "\n",
    "#### URL Matching based Method\n",
    "- Feature Extraction: extract article url and image url from provided file\n",
    "- remove manually defined stop words from urls\n",
    "- URL tokenization\n",
    "- URL comparison: a pair of image url and article url is considered to be matched if they contains more than one common tokens.\n",
    "- sort the potential matched image list by the number of same tokens\n",
    "- Evaluate performance of this URL Matching based Method using MR100 on both training dataset and validation dataset\n",
    "\n",
    "#### Image Captioning based Model\n",
    "- acquired image caption from the pre-trained image captioning model (https://github.com/ruotianluo/ImageCaptioning.pytorch)\n",
    "- caculate the wmd between the each pair of image caption and article title\n",
    "- sort the potential matched image list by the wmd\n",
    "- Evaluate performance of this Image Captioning based Method using MR100 on both training dataset and validation dataset\n",
    "\n",
    "#### Face Matching\n",
    "##### Step 1:  Create a specific training dataset for face-name matching\n",
    "- A pair of article and image is used for this model training if the pair satisfies following two conditions: 1. the title of article include person's name, 2. the image is a human face image\n",
    "- Extract person's name from article title\n",
    "- Remove the image from this specific traning dataset if face can't be detected using multiple face detection frameworks\n",
    "- Build connections between the extracted name and the corresponding human face image\n",
    "- If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website.\n",
    "\n",
    "##### Step 2: Face Name Matching\n",
    "- Extract the person's names from testing article titles\n",
    "- Find the corresponding face images from the training dataset which created in step 1\n",
    "- Encode the face images into feature vectors\n",
    "- Compare the corresponding face images with each test face image by caculating the cosine distance between two feature vectors\n",
    "- Two face images are regared as matched if the cosine distance between two vectors is smaller or equal to 0.4 \n",
    "- Sort the potential matcheing image list by both cosine distance and total matches\n",
    "\n",
    "##### Step 3:Evaluation\n",
    "\n",
    "##### Step 4: Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71527da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_wmd_sim(wmd_sim_file, sim_result):\n",
    "    f = open(wmd_sim_file, \"a\")\n",
    "    for key, v in sim_result.items():\n",
    "        for item in v:\n",
    "            result=key+\"\\t\"+os.path.basename(item[0])+\"\\t\"+str(item[1])+\"\\n\"\n",
    "            f.write(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c98fb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def img_cap_similarity(caption_file, title_file, wmd_sim_file, title_eng_idx):\n",
    "    caption_dict=get_caption (caption_file)\n",
    "    ar_id_title=get_ar_id_title (title_file, title_eng_idx)\n",
    "    sim_result=cal_sim(ar_id_title, caption_dict)\n",
    "    write_wmd_sim(wmd_sim_file, sim_result)\n",
    "    lines_caption_sim = []\n",
    "    with open(wmd_sim_file) as f:\n",
    "        lines_caption_sim = f.readlines()\n",
    "    ar_cap_sim_dic={}\n",
    "    for line in lines_caption_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_cap_sim_dic:\n",
    "            ar_cap_sim_dic[segs[0]]={}\n",
    "        ar_cap_sim_dic[segs[0]][segs[1]]= -float(segs[2])\n",
    "    sort_final_result=sort_dict(ar_cap_sim_dic)\n",
    "    return sort_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23c5b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(input):\n",
    "    result={}\n",
    "    for k, v in input.items():\n",
    "        sort_v=dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k]=sort_v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e00f1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cap_sim (sort_final_result):\n",
    "    count=0\n",
    "    for key, value in sort_final_result.items():\n",
    "        first_tuple_elements=[]\n",
    "        for a_tuple in value:\n",
    "            first_tuple_elements.append(a_tuple)\n",
    "        if key in first_tuple_elements[0:100]:\n",
    "            count+=1\n",
    "    return count/len(sort_final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b16608",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = img_cap_similarity(r\"processed_data\\data\\train_image_caption_result.txt\", \\\n",
    "                                  r\"processed_data\\data\\train_title_eng.tsv\",\\\n",
    "                                  r\"result/tr_caption_sim_wmd.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b950bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_final_result = img_cap_similarity(r\"processed_data\\data\\eval_image_caption_result.txt\", \\\n",
    "                                 \"processed_data\\data\\eval_title_eng.tsv\",\\\n",
    "                                \"result/eval_caption_sim_wmd.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "700008ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042312526183493925\n"
     ]
    }
   ],
   "source": [
    "print(eval_cap_sim (sort_final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = img_cap_similarity(r\"processed_data\\data\\test_image_caption_result.txt\", \\\n",
    "                                 \"processed_data\\data\\test_title_eng.tsv\",\\\n",
    "                                \"result/test_caption_sim_wmd.tsv\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a1038",
   "metadata": {},
   "source": [
    "## Face Matching\n",
    "### Step 1:  Create a specific training dataset for face-name matching\n",
    "- A pair of article and image is used for this model training if the pair satisfies following two conditions: 1. the title of article include person's name, 2. the image is a human face image\n",
    "- Extract person's name from article title\n",
    "- Remove the image from this specific traning dataset if face can't be detected using multiple face detection frameworks\n",
    "- Build connections between the extracted name and the corresponding human face image\n",
    "- If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website.\n",
    "### Step 2: Face Name Matching\n",
    "- Extract the person's names from testing article titles\n",
    "- Find the corresponding face images from the training dataset which created in step 1\n",
    "- Encode the face images into feature vectors\n",
    "- Compare the corresponding face images with each test face image by caculating the cosine distance between two feature vectors\n",
    "- Two face images are regared as matched if the cosine distance between two vectors is smaller or equal to 0.4 \n",
    "- Sort the potential matcheing image list by both cosine distance and total matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fae90",
   "metadata": {},
   "source": [
    "### Step 1:  Create a specific training dataset for face-name matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920d8f9",
   "metadata": {},
   "source": [
    "#### Name Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb20189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import icrawler\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import warnings\n",
    "from deepface import DeepFace\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = r\"C:\\Users\\yuxia\\Documents\\java-se-8u41-ri\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger(r'C:\\Users\\yuxia\\Downloads\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\classifiers\\english.all.3class.distsim.crf.ser.gz',\n",
    "                           r'C:\\Users\\yuxia\\Downloads\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\stanford-ner.jar',\n",
    "                           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_name(classified_text):\n",
    "    i=0\n",
    "    name_list=[]\n",
    "    while i < len(classified_text)-1:\n",
    "        if classified_text[i][1] == 'PERSON':\n",
    "            name = classified_text[i][0]\n",
    "            if classified_text[i+1][1]=='PERSON':\n",
    "                name+=\" \"+classified_text[i+1][0]\n",
    "                i+=1\n",
    "            name_list.append(name)\n",
    "        i+=1\n",
    "    if i == len(classified_text)-1 and classified_text[i][1] == 'PERSON':\n",
    "        name_list.append(classified_text[i][0])\n",
    "    return name_list\n",
    "\n",
    "\n",
    "def add_title_name(tr_file, output_file):\n",
    "    a_file = open(tr_file, encoding=\"utf8\")\n",
    "    next(a_file)\n",
    "    cnt=0\n",
    "    header=\"img_id\"+\"\\t\"+\"title\"+\"\\t\"+\"title_eng\"+\"\\t\"+\"title_names\"\n",
    "    with open(output_file, 'a',encoding=\"utf-8\") as the_file:\n",
    "        for line in a_file:\n",
    "            title_eng=line.split(\"\\t\")[2]\n",
    "            tokenized_text = word_tokenize(title_eng)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            names=concat_name(classified_text)\n",
    "            if len(names)>0:\n",
    "                names_str = ','.join(names)\n",
    "                print(names_str)\n",
    "                new_line=line.strip(\"\\n\")+\"\\t\"+names_str+\"\\n\"\n",
    "                cnt+=1\n",
    "            else:\n",
    "                new_line=line.strip(\"\\n\")+\"\\t \"+\"\\n\"\n",
    "            the_file.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_title_eng_file = r'processed_data\\data\\train_title_eng.tsv'\n",
    "eval_title_eng_file = r'processed_data\\data\\eval_title_eng.tsv'\n",
    "test_title_eng_file = r'processed_data\\data\\test_title_eng.tsv'\n",
    "tr_title_eng_name_file = r'processed_data\\data\\train_title_eng_name.tsv'\n",
    "eval_title_eng_name_file = r'processed_data\\data\\eval_title_eng_name.tsv'\n",
    "test_title_eng_name_file = r'processed_data\\data\\test_title_eng_name.tsv'\n",
    "add_title_name(tr_title_eng_file, output_file)\n",
    "add_title_name(eval_title_eng_file, output_file)\n",
    "add_title_name(test_title_eng_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=[r\"processed_data\\data\\train_title_eng_name.tsv\", r\"processed_data\\data\\eval_title_eng_name.tsv\"]\n",
    "output_file=r\"processed_data\\data\\train_eval_title_eng_name.tsv\"\n",
    "combine_files(filenames, output_file, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2f0ca",
   "metadata": {},
   "source": [
    "#### Processing the face image in the training/validation dataset\n",
    "- group images by the extracted names in the corresponding article\n",
    "- Created name indexing image dictionaries to fit unicode convention in OpenCV\n",
    "Note: OpenCV only accepts ASCII characters for image paths when reading and writing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7601828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_folder(name_file, train_face_folder, train_img_folder):\n",
    "    a_file = open(name_file, encoding=\"utf8\")\n",
    "    next(a_file)\n",
    "    for line in a_file:\n",
    "        line = line.strip(\"\\n\")\n",
    "        img_name = line.split(\"\\t\")[0]\n",
    "        names = line.split(\"\\t\")[4].rstrip()\n",
    "        if len(names) > 0:\n",
    "            path = os.path.join(train_face_folder, names.split(\",\")[0])\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            if os.path.exists(os.path.join(train_img_folder, img_name)):\n",
    "                copyfile(os.path.join(train_img_folder, img_name), os.path.join(path, img_name))\n",
    "\n",
    "def create_mapped_folder(d, mapped_folder):\n",
    "    if not os.path.isdir(mapped_folder):\n",
    "        os.mkdir(mapped_folder)\n",
    "    sub_directories = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    idx_name = {}\n",
    "    name_idx = {}\n",
    "    idx = 1\n",
    "    for sub_dir in sub_directories:\n",
    "        idx_name[sub_dir] = 'face_' + str(idx)\n",
    "        name_idx['face_' + str(idx)] = sub_dir\n",
    "        idx += 1\n",
    "    sub_full_paths = [os.path.join(d, o) for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    for sub_dir in sub_full_paths:\n",
    "         mapped_file_folder(sub_dir, mapped_folder, idx_name)\n",
    "    return idx_name, name_idx\n",
    "\n",
    "def mapped_file_folder(src, dest, idx_name):\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        dest_folder = os.path.join(dest, idx_name[os.path.basename(src)])\n",
    "        if not os.path.isdir(dest_folder):\n",
    "            os.mkdir(dest_folder)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_folder(r'processed_data\\data\\train_eval_title_eng_name.tsv', r\"processed_data\\img\\train_eval_faces\", r\"img\\train\")\n",
    "create_name_folder(r'processed_data\\data\\train_eval_title_eng_name.tsv', r\"processed_data\\img\\train_eval_faces\", r\"img\\eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_folder(r'processed_data\\data\\train_title_eng_name.tsv', r\"processed_data\\img\\train_faces\", r\"img\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eval_idx_name, tr_eval_name_idx = create_mapped_folder(r\"processed_data\\img\\train_eval_faces\", r\"processed_data\\img\\train_eval_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx_name, tr_name_idx = create_mapped_folder(r\"processed_data\\img\\train_faces\", r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d3c4c",
   "metadata": {},
   "source": [
    "- Remove non-face images from this specific training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_cv( file):\n",
    "    # Load the cascade\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    # Read the input image\n",
    "    img = cv2.imread(file)\n",
    "    # Convert into grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    width, height = gray.shape\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    # Draw rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        if w != width or height != h:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def deep_detect_backend(file):\n",
    "    backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface']\n",
    "    c = 0\n",
    "    for backend in backends:\n",
    "        try:\n",
    "            detected_face = DeepFace.detectFace(file, detector_backend=backend)\n",
    "        except:\n",
    "            c += 1\n",
    "    if c == len(backends):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def deep_detect(file):\n",
    "    models = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "    c = 0\n",
    "    for model in models:\n",
    "        try:\n",
    "            detected_face = DeepFace.detectFace(file, model_name=model)\n",
    "        except:\n",
    "            c += 1\n",
    "    if c == len(models):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_face_img(path):\n",
    "    face_img=[]\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    cnt=0\n",
    "    for sub_dir in sub_directories:\n",
    "        print(os.path.basename(sub_dir))\n",
    "        files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "        for file in files:\n",
    "            if deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file):\n",
    "                face_img.append(os.path.basename(file))\n",
    "            else:\n",
    "                os.remove(file)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_face_img_list=remove_no_face_img(r\"processed_data\\img\\train_eval_mapped_face\")\n",
    "train_face_img_list=remove_no_face_img(r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c059e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_folder(path):\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    for sub_dir in sub_directories:\n",
    "            files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "            if len(files)==0:\n",
    "                shutil.rmtree(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_empty_folder(r\"processed_data\\img\\train_eval_mapped_face\")\n",
    "remove_empty_folder(r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mapping(d):\n",
    "    sub_directories = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    idx_name = {}\n",
    "    name_idx = {}\n",
    "    idx = 1\n",
    "    for sub_dir in sub_directories:\n",
    "        idx_name[sub_dir] = 'face_' + str(idx)\n",
    "        name_idx['face_' + str(idx)] = sub_dir\n",
    "        idx += 1\n",
    "    return idx_name, name_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eval_idx_name, tr_eval_name_idx = find_mapping(r\"processed_data\\img\\train_eval_faces\")\n",
    "tr_idx_name, tr_name_idx = find_mapping(r\"processed_data\\img\\train_faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_eval_idx_name))\n",
    "print(len(tr_idx_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0e862",
   "metadata": {},
   "source": [
    "- Find the list of news articles with title including person's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar_name_list (article_file, title_eng_idx):\n",
    "    articles_names=open(article_file, 'r', encoding=\"utf-8\")\n",
    "    next(articles_names)\n",
    "    lines = [line.strip() for line in articles_names]\n",
    "    result=[]\n",
    "    for i in range(len(lines)):\n",
    "        orig_line=lines[i]\n",
    "        segs = orig_line.split(\"\\t\")\n",
    "        if len(segs) > title_eng_idx and len(segs[len(segs)-1].strip())>0 and segs[len(segs)-1].strip()!='NA':\n",
    "            result.append((segs[0], segs[title_eng_idx].split(\",\")[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4705cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ar_name_list=get_ar_name_list(r\"processed_data\\data\\train_title_eng_name.tsv\", 4)\n",
    "eval_ar_name_list=get_ar_name_list(r\"processed_data\\data\\eval_title_eng_name.tsv\", 4)\n",
    "test_ar_name_list=get_ar_name_list(r\"processed_data\\data\\test_title_eng_name.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ar_name_list))\n",
    "print(len(eval_ar_name_list))\n",
    "print(len(test_ar_name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305dc5c7",
   "metadata": {},
   "source": [
    "#### Face Image Crawling\n",
    "If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw_missing_images(ar_name_list, idx_name, train_mapped_face_path, crawl_face_path):\n",
    "    if not os.path.isdir(crawl_face_path):\n",
    "        os.mkdir(crawl_face_path)\n",
    "    for ar_name in ar_name_list:\n",
    "        if ar_name[1] in idx_name and ar_name[1] in idx_name and os.path.exists(os.path.join(train_mapped_face_path, idx_name[ar_name[1]])):\n",
    "                print(\"found\")\n",
    "        else:\n",
    "             \n",
    "            if not os.path.isdir(os.path.join(crawl_face_path, ar_name[1])):\n",
    "                os.mkdir(os.path.join(crawl_face_path, ar_name[1]))\n",
    "            google_crawler = GoogleImageCrawler(feeder_threads=1,parser_threads=2,downloader_threads=4,storage={'root_dir': os.path.join(crawl_face_path, ar_name[1])})\n",
    "            filters = dict(date=((2019, 1, 1), (2021, 7, 30)))\n",
    "            google_crawler.crawl(keyword=ar_name[1], filters=filters, max_num=5, file_idx_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ee8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw_missing_images(train_ar_name_list, {}, \"\",  r\"processed_data\\img\\crawl_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw_missing_images(eval_ar_name_list, tr_idx_name, r\"processed_data\\img\\train_mapped_face\", r\"processed_data\\img\\crawl_train_face\")\n",
    "craw_missing_images(test_ar_name_list, tr_eval_idx_name, r\"processed_data\\img\\train_eval_mapped_face\", r\"processed_data\\img\\crawl_train_eval_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_eval_idx_name, crawl_tr_eval_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_train_eval_face\", \\\n",
    "                                                                    r\"processed_data\\img\\crawl_train_eval_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_idx_name, crawl_tr_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_train_face\", \\\n",
    "                                                          r\"processed_data\\img\\crawl_train_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30974dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_idx_name, crawl_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_face\", \\\n",
    "                                                          r\"processed_data\\img\\crawl_face_mapped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1b2d6",
   "metadata": {},
   "source": [
    "- Remove non-face images from crawled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_face_img_crawl(path):\n",
    "    face_img=[]\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    cnt=0\n",
    "    for sub_dir in sub_directories:\n",
    "        print(os.path.basename(sub_dir))\n",
    "        files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "        for file in files:\n",
    "            if os.path.isfile(file) and (deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file)):\n",
    "                face_img.append(os.path.basename(file))\n",
    "            elif os.path.isdir (sub_dir):\n",
    "                shutil.rmtree(sub_dir)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd354f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_train_eval_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d57b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_train_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_eval_idx_name, crawl_tr_eval_name_idx = find_mapping(r\"processed_data\\img\\crawl_train_eval_face\")\n",
    "crawl_tr_idx_name, crawl_tr_name_idx = find_mapping(r\"processed_data\\img\\crawl_train_face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266d4b1",
   "metadata": {},
   "source": [
    "### Face Name Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e49ad4",
   "metadata": {},
   "source": [
    "#### Image Candidate Selection\n",
    "Only face images are selected to match with the images in training set created step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_face_image(src_path, dst_path):\n",
    "    if not os.path.isdir(dst_path):\n",
    "                os.mkdir(dst_path)\n",
    "    files = [os.path.join(src_path, f) for f in os.listdir(src_path) if f.endswith('.jpg')]\n",
    "    for file in files:\n",
    "        if deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file):\n",
    "            shutil.copy(file, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_face_image(r'img\\train', r'processed_data\\img\\train_face_candidate')\n",
    "select_face_image(r'img\\eval', r'processed_data\\img\\eval_face_candidate')\n",
    "select_face_image(r'img\\test', r'processed_data\\img\\test_face_candidate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac06e1",
   "metadata": {},
   "source": [
    "### caculate image similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "def get_face_similarity(face_img_candidate_dir, train_mapped_img_dir, ar_name_list, idx_name):\n",
    "    cnt = 0\n",
    "    record = 0\n",
    "    img_files = [f for f in os.listdir(face_img_candidate_dir) if f.endswith('.jpg')]\n",
    "    print(len(img_files))\n",
    "    ar_img_files = {}\n",
    "    for ar_name in ar_name_list:\n",
    "        if ar_name[1].strip() != 'NA' and ar_name[1] in idx_name:\n",
    "            img_db_path=\"\"\n",
    "            if os.path.exists(os.path.join(train_mapped_img_dir, idx_name[ar_name[1]])):\n",
    "                img_db_path=os.path.join(train_mapped_img_dir, idx_name[ar_name[1]])\n",
    "            if len(img_db_path)>0:\n",
    "                df_results = []\n",
    "                t = time.process_time()\n",
    "                count = 0\n",
    "                for img_file in img_files:\n",
    "                    img_path = os.path.join(face_img_candidate_dir, img_file)\n",
    "                    df = DeepFace.find(img_path=img_path, db_path=img_db_path,\n",
    "                                       model_name='Facenet', enforce_detection=False)\n",
    "                    if len(df) > 0:\n",
    "                        df_results.append((img_path, df['Facenet_cosine'].mean(), len(df)))\n",
    "                    else:\n",
    "                        df_results.append((img_path, \"NA\", 0))\n",
    "                    count += 1\n",
    "                ar_img_files[ar_name[0]] = df_results\n",
    "                cnt += 1\n",
    "                elapsed_time = time.process_time() - t\n",
    "\n",
    "                print(str(datetime.now()))\n",
    "                print(\"in \", elapsed_time, \"seconds complete\", cnt, \" name completed\", \" compared with \", count,\n",
    "                      \"images\")\n",
    "        record += 1\n",
    "        print(\"processing \", record, \" files\")\n",
    "    return ar_img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cbbb1",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_face_matching_similarity(output_file, ar_img_files):\n",
    "    f = open(output_file, \"a\")\n",
    "    for key, v in ar_img_files.items():\n",
    "        for item in v:\n",
    "            result = key + \"\\t\" + os.path.basename(item[0]) + \"\\t\" + str(item[1]) + \"\\t\" + str(item[2]) + \"\\n\"\n",
    "            f.write(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dictionary(input_dict):\n",
    "    result={}\n",
    "    for k, v in input_dict.items():\n",
    "        sort_v=dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k]=sort_v\n",
    "    return result\n",
    "\n",
    "def cal_face_matching_similarity(input_file):\n",
    "    image_train_sim = []\n",
    "    with open(input_file) as f:\n",
    "        image_train_sim = f.readlines()\n",
    "    ar_train_sim_dic={}\n",
    "    for line in image_train_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_train_sim_dic:\n",
    "            ar_train_sim_dic[segs[0]]=[]\n",
    "        ar_train_sim_dic[segs[0]].append((segs[1], segs[2], segs[3]))\n",
    "    ar_train_sim_dic_cal={}\n",
    "    for k, v in ar_train_sim_dic.items():\n",
    "        if k not in ar_train_sim_dic_cal:\n",
    "            ar_train_sim_dic_cal[k]={}\n",
    "        for item in v:\n",
    "            if int(item[2])==0:\n",
    "                sim=0\n",
    "            else:\n",
    "                sim=(1-float(item[1]))*int(item[2])\n",
    "            ar_train_sim_dic_cal[k][item[0]]= sim\n",
    "    return sort_dictionary(ar_train_sim_dic_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d43a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ar_img_files = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\train_mapped_face', eval_ar_name_list, tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e199bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/eval_face_similarity.tsv\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ar_img_files_crawl = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\crawl_train_face_mapped', eval_ar_name_list, crawl_tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/eval_face_similarity_crawl.tsv\",eval_ar_img_files_crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb5ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_face_matching = cal_face_matching_similarity(r\"result/eval_face_similarity.tsv\")\n",
    "eval_face_matching_crawl = cal_face_matching_similarity(r\"result/eval_face_similarity_crawl.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d012f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_MR(eval_face_matching):\n",
    "    count=0\n",
    "    for key, value in eval_face_matching.items():\n",
    "        first_tuple_elements=[]\n",
    "        for a_tuple in value:\n",
    "            first_tuple_elements.append(a_tuple)\n",
    "        if key in first_tuple_elements[0:100]:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb6dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21052631578947367\n",
      "0.0067085953878406705\n"
     ]
    }
   ],
   "source": [
    "print(cal_MR(eval_face_matching)/len(eval_face_matching))\n",
    "print(cal_MR(eval_face_matching)/2385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27fe665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10404624277456648\n",
      "0.007547169811320755\n"
     ]
    }
   ],
   "source": [
    "print(cal_MR(eval_face_matching_crawl)/len(eval_face_matching_crawl))\n",
    "print(cal_MR(eval_face_matching_crawl)/2385)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457d87a",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baedf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_img_files = get_face_similarity(r'processed_data\\img\\test_face_candidate',\n",
    "                                   r'processed_data\\img\\train_eval_mapped_face', \n",
    "                                        test_ar_name_list, \n",
    "                                        tr_eval_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/test_train_img_similarity.txt\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_img_files_crawl = get_face_similarity(r'processed_data\\img\\test_face_candidate',\n",
    "                                   r'processed_data\\img\\crawl_train_eval_face_mapped', \n",
    "                                              test_ar_name_list, \n",
    "                                              tr_eval_crawl_tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964606f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/test_crawl_img_similarity.txt\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc715e9",
   "metadata": {},
   "source": [
    "### Model Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f8a88",
   "metadata": {},
   "source": [
    "convert a file into a dictionary respresenting results from image captioning based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "692bbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_caption_result (caption_sim_wmd_file):\n",
    "    lines_caption_sim = []\n",
    "    with open(caption_sim_wmd_file) as f:\n",
    "        lines_caption_sim = f.readlines()\n",
    "    ar_cap_sim_dic={}\n",
    "    for line in lines_caption_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_cap_sim_dic:\n",
    "            ar_cap_sim_dic[segs[0]]={}\n",
    "        ar_cap_sim_dic[segs[0]][os.path.splitext(segs[1])[0]]= 1-float(segs[2])\n",
    "    return ar_cap_sim_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b802a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_cap_sim_dic=cal_caption_result(r\"result\\test_caption_sim_wmd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6a14046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n"
     ]
    }
   ],
   "source": [
    "print(len(ar_cap_sim_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a9d1b",
   "metadata": {},
   "source": [
    "convert a file into a dictionary respresenting results from face name matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c780c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_matching_result (face_matching_file):\n",
    "    image_train_sim = []\n",
    "    with open(face_matching_file) as f:\n",
    "        image_train_sim = f.readlines()\n",
    "    ar_train_sim_dic={}\n",
    "    for line in image_train_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_train_sim_dic:\n",
    "            ar_train_sim_dic[segs[0]]=[]\n",
    "        ar_train_sim_dic[segs[0]].append((os.path.splitext(segs[1])[0], segs[2], segs[3]))\n",
    "    ar_train_sim_dic_cal={}\n",
    "    \n",
    "    for k, v in ar_train_sim_dic.items():\n",
    "        if k not in ar_train_sim_dic_cal:\n",
    "            ar_train_sim_dic_cal[k]={}\n",
    "        for item in v:\n",
    "            if int(item[2])==0:\n",
    "                sim=0\n",
    "            else:\n",
    "                sim=(1-float(item[1]))*int(item[2])\n",
    "            ar_train_sim_dic_cal[k][item[0]]= sim\n",
    "    return ar_train_sim_dic_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4d2c8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_train_sim_dic_cal=face_matching_result(r\"result\\test_train_img_similarity.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "facafc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_crawl_sim_dic_cal=face_matching_result(r\"result\\test_crawl_img_similarity.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "65a4a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "print(len(ar_crawl_sim_dic_cal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146406a2",
   "metadata": {},
   "source": [
    "normalize value of a given dictionary (Min-max normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a0d9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dict(a_dict):\n",
    "    result={}\n",
    "    amin, amax = min(a_dict.values()), max(a_dict.values())\n",
    "    for k, v in a_dict.items():\n",
    "        if amax-amin==0:\n",
    "            result[k]=0\n",
    "        else:\n",
    "            result[k] = (v-amin) / (amax-amin)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d42278",
   "metadata": {},
   "source": [
    "normalize value of a given dictionary \n",
    "the dictionary respresents results from face name matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "76aafec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sim(a_dict):\n",
    "    result={}\n",
    "    print(len(a_dict))\n",
    "    for k, v in a_dict.items():\n",
    "        result[k]=norm_dict(v)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824293d6",
   "metadata": {},
   "source": [
    "sort the value in the given dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b06e99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(a_dict):\n",
    "    normalized_dict=norm_sim(a_dict)\n",
    "    result = {}\n",
    "    for k, v in normalized_dict.items():\n",
    "        sort_v = dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k] = sort_v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55846f5d",
   "metadata": {},
   "source": [
    "sort dictionary respresenting results from face name matching based model\n",
    "sort dictionary respresenting results from image captioning matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0392e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n",
      "128\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "cap_dict=sort_dict(ar_cap_sim_dic)\n",
    "train_dict=sort_dict(ar_train_sim_dic_cal)\n",
    "crawl_dict=sort_dict(ar_crawl_sim_dic_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba61773",
   "metadata": {},
   "source": [
    "merge results from image captioning based model and results from face matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fd62c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cap_face(cap_dict, train_dict, crawl_dict, weight_cap, weight_img):\n",
    "    result={}\n",
    "    for k, v in cap_dict.items():\n",
    "        img_id=os.path.splitext(k)[0]\n",
    "        result[img_id]=v\n",
    "        if k in train_dict:\n",
    "            for k_tr, v_tr in train_dict[k].items():\n",
    "                result[img_id][k_tr]=v_tr*weight_img\n",
    "        if k in crawl_dict:\n",
    "            for k_cr, v_cr in crawl_dict[k].items():\n",
    "                result[img_id][k_cr]=v_cr*weight_img\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1f74b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_face_result=merge_cap_face(cap_dict, train_dict, crawl_dict, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ba345398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n"
     ]
    }
   ],
   "source": [
    "sorted_cap_face_result=sort_dict(cap_face_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37471b04",
   "metadata": {},
   "source": [
    "truncate image candidates into top 100 list for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "07169a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_result(a_dict):\n",
    "    for key, value in a_dict.items():\n",
    "        l = [*value]\n",
    "        l_100 = l[0:100]\n",
    "        a_dict[key]=l_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2238a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_result(sorted_cap_face_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "65694295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_cap_face_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86493cbb",
   "metadata": {},
   "source": [
    "acquire result from url matching based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a19be005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching url\n",
      "1772\n",
      "1772\n"
     ]
    }
   ],
   "source": [
    "img_id_name_dict = extract_img_url_token(\"../data/MediaEvalNewsImagesBatch04images.tsv\",\n",
    "                                              TEST_I_ID_IDX,\n",
    "                                              TEST_IMG_URL_IDX)\n",
    "article_id_name_dict = extract_article_token(\"../data/MediaEvalNewsImagesBatch04articles.tsv\", \n",
    "                                             TEST_A_ID_IDX,\n",
    "                                             TEST_TITLE_IDX)\n",
    "url_result = match_url(article_id_name_dict, img_id_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19dd42e",
   "metadata": {},
   "source": [
    "merge result from url matching based model into result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ff57c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_url(final_cap_face_result, url_result):\n",
    "    final_result={}\n",
    "    for s_id, s_value in final_cap_face_result.items():\n",
    "        if s_id in result:\n",
    "            diff_elements = [x for x in url_result[s_id] if x not in s_value ]\n",
    "            common_elements= [x for x in s_value if x in url_result[s_id] ]\n",
    "            tail_elements=s_value[len(s_value)-len(diff_elements):]\n",
    "            common_ele_in_tail=[x for x in common_elements if x in tail_elements]\n",
    "            if len(common_ele_in_tail)>0:\n",
    "                new_value=diff_elements+common_ele_in_tail+s_value[:len(s_value)-len(diff_elements)-len(common_ele_in_tail)]\n",
    "            else:\n",
    "                new_value=diff_elements+s_value[:len(s_value)-len(diff_elements)]\n",
    "            final_result[s_id]= new_value\n",
    "\n",
    "        else:\n",
    "            final_result[s_id]=s_value\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8a34c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=merge_url(sorted_cap_face_result, url_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069b19a",
   "metadata": {},
   "source": [
    "save final result into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb1a223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_result(output_file, final_result):\n",
    "    with open(output_file, \"w\") as the_file:\n",
    "        header=\"particleID\"\n",
    "        for i in range(100):\n",
    "            header+=\"\\t\"+\"iid\"+str(i+1)\n",
    "        the_file.write(header+\"\\n\")\n",
    "        for key, value in final_result.items():\n",
    "            the_file.write(key+'\\t'+ \"\\t\".join(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0be196e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_final_result(r\"result\\final_result.tsv\", final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f2c40",
   "metadata": {},
   "source": [
    "Result Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bc4e2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"result\\final_result.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f04f3b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>particleID</th>\n",
       "      <th>iid1</th>\n",
       "      <th>iid2</th>\n",
       "      <th>iid3</th>\n",
       "      <th>iid4</th>\n",
       "      <th>iid5</th>\n",
       "      <th>iid6</th>\n",
       "      <th>iid7</th>\n",
       "      <th>iid8</th>\n",
       "      <th>iid9</th>\n",
       "      <th>...</th>\n",
       "      <th>iid91</th>\n",
       "      <th>iid92</th>\n",
       "      <th>iid93</th>\n",
       "      <th>iid94</th>\n",
       "      <th>iid95</th>\n",
       "      <th>iid96</th>\n",
       "      <th>iid97</th>\n",
       "      <th>iid98</th>\n",
       "      <th>iid99</th>\n",
       "      <th>iid100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000265260</td>\n",
       "      <td>134746</td>\n",
       "      <td>134710</td>\n",
       "      <td>135977</td>\n",
       "      <td>134622</td>\n",
       "      <td>134853</td>\n",
       "      <td>136039</td>\n",
       "      <td>136139</td>\n",
       "      <td>136193</td>\n",
       "      <td>134762</td>\n",
       "      <td>...</td>\n",
       "      <td>134315</td>\n",
       "      <td>134322</td>\n",
       "      <td>134510</td>\n",
       "      <td>135693</td>\n",
       "      <td>134997</td>\n",
       "      <td>135109</td>\n",
       "      <td>135698</td>\n",
       "      <td>134633</td>\n",
       "      <td>136319</td>\n",
       "      <td>134726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001935289</td>\n",
       "      <td>135908</td>\n",
       "      <td>135628</td>\n",
       "      <td>134775</td>\n",
       "      <td>136331</td>\n",
       "      <td>134390</td>\n",
       "      <td>136306</td>\n",
       "      <td>134381</td>\n",
       "      <td>134409</td>\n",
       "      <td>134909</td>\n",
       "      <td>...</td>\n",
       "      <td>136045</td>\n",
       "      <td>136458</td>\n",
       "      <td>136137</td>\n",
       "      <td>134266</td>\n",
       "      <td>134868</td>\n",
       "      <td>135007</td>\n",
       "      <td>135435</td>\n",
       "      <td>136062</td>\n",
       "      <td>136172</td>\n",
       "      <td>136179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002375244</td>\n",
       "      <td>136277</td>\n",
       "      <td>134791</td>\n",
       "      <td>136639</td>\n",
       "      <td>134624</td>\n",
       "      <td>134416</td>\n",
       "      <td>135315</td>\n",
       "      <td>135770</td>\n",
       "      <td>136169</td>\n",
       "      <td>136530</td>\n",
       "      <td>...</td>\n",
       "      <td>135374</td>\n",
       "      <td>135406</td>\n",
       "      <td>135453</td>\n",
       "      <td>135626</td>\n",
       "      <td>135924</td>\n",
       "      <td>135952</td>\n",
       "      <td>136016</td>\n",
       "      <td>136106</td>\n",
       "      <td>136168</td>\n",
       "      <td>136189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002735288</td>\n",
       "      <td>136819</td>\n",
       "      <td>136242</td>\n",
       "      <td>134962</td>\n",
       "      <td>134606</td>\n",
       "      <td>136361</td>\n",
       "      <td>134390</td>\n",
       "      <td>134123</td>\n",
       "      <td>134231</td>\n",
       "      <td>134332</td>\n",
       "      <td>...</td>\n",
       "      <td>136561</td>\n",
       "      <td>134322</td>\n",
       "      <td>136431</td>\n",
       "      <td>134502</td>\n",
       "      <td>134709</td>\n",
       "      <td>136243</td>\n",
       "      <td>134149</td>\n",
       "      <td>134162</td>\n",
       "      <td>134733</td>\n",
       "      <td>134788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002835245</td>\n",
       "      <td>135405</td>\n",
       "      <td>136751</td>\n",
       "      <td>134806</td>\n",
       "      <td>135863</td>\n",
       "      <td>136083</td>\n",
       "      <td>135390</td>\n",
       "      <td>136320</td>\n",
       "      <td>136390</td>\n",
       "      <td>134853</td>\n",
       "      <td>...</td>\n",
       "      <td>135534</td>\n",
       "      <td>135709</td>\n",
       "      <td>134206</td>\n",
       "      <td>136624</td>\n",
       "      <td>136931</td>\n",
       "      <td>136639</td>\n",
       "      <td>136288</td>\n",
       "      <td>134634</td>\n",
       "      <td>134627</td>\n",
       "      <td>135606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>1999075246</td>\n",
       "      <td>135716</td>\n",
       "      <td>134131</td>\n",
       "      <td>135146</td>\n",
       "      <td>135009</td>\n",
       "      <td>135268</td>\n",
       "      <td>135742</td>\n",
       "      <td>135805</td>\n",
       "      <td>136327</td>\n",
       "      <td>134737</td>\n",
       "      <td>...</td>\n",
       "      <td>137028</td>\n",
       "      <td>135043</td>\n",
       "      <td>134556</td>\n",
       "      <td>134699</td>\n",
       "      <td>134729</td>\n",
       "      <td>134770</td>\n",
       "      <td>134923</td>\n",
       "      <td>135112</td>\n",
       "      <td>135368</td>\n",
       "      <td>135506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>1999165241</td>\n",
       "      <td>134482</td>\n",
       "      <td>135136</td>\n",
       "      <td>135987</td>\n",
       "      <td>136178</td>\n",
       "      <td>136355</td>\n",
       "      <td>136646</td>\n",
       "      <td>136889</td>\n",
       "      <td>136763</td>\n",
       "      <td>134782</td>\n",
       "      <td>...</td>\n",
       "      <td>136791</td>\n",
       "      <td>135627</td>\n",
       "      <td>136258</td>\n",
       "      <td>134220</td>\n",
       "      <td>134356</td>\n",
       "      <td>136583</td>\n",
       "      <td>136668</td>\n",
       "      <td>134291</td>\n",
       "      <td>136412</td>\n",
       "      <td>136326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>1999345240</td>\n",
       "      <td>136293</td>\n",
       "      <td>135876</td>\n",
       "      <td>135329</td>\n",
       "      <td>136288</td>\n",
       "      <td>136356</td>\n",
       "      <td>135714</td>\n",
       "      <td>134205</td>\n",
       "      <td>134395</td>\n",
       "      <td>135003</td>\n",
       "      <td>...</td>\n",
       "      <td>136679</td>\n",
       "      <td>136669</td>\n",
       "      <td>134933</td>\n",
       "      <td>136116</td>\n",
       "      <td>135566</td>\n",
       "      <td>136096</td>\n",
       "      <td>135390</td>\n",
       "      <td>135844</td>\n",
       "      <td>136285</td>\n",
       "      <td>134557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>1999355239</td>\n",
       "      <td>134193</td>\n",
       "      <td>136200</td>\n",
       "      <td>135183</td>\n",
       "      <td>135391</td>\n",
       "      <td>135705</td>\n",
       "      <td>135853</td>\n",
       "      <td>135432</td>\n",
       "      <td>135722</td>\n",
       "      <td>135813</td>\n",
       "      <td>...</td>\n",
       "      <td>136327</td>\n",
       "      <td>134336</td>\n",
       "      <td>134523</td>\n",
       "      <td>136558</td>\n",
       "      <td>136988</td>\n",
       "      <td>134946</td>\n",
       "      <td>135912</td>\n",
       "      <td>136476</td>\n",
       "      <td>134380</td>\n",
       "      <td>134397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>1999735294</td>\n",
       "      <td>136898</td>\n",
       "      <td>134284</td>\n",
       "      <td>135317</td>\n",
       "      <td>135973</td>\n",
       "      <td>136439</td>\n",
       "      <td>136295</td>\n",
       "      <td>136588</td>\n",
       "      <td>137181</td>\n",
       "      <td>136322</td>\n",
       "      <td>...</td>\n",
       "      <td>135127</td>\n",
       "      <td>136193</td>\n",
       "      <td>135885</td>\n",
       "      <td>135618</td>\n",
       "      <td>135641</td>\n",
       "      <td>136151</td>\n",
       "      <td>137029</td>\n",
       "      <td>134974</td>\n",
       "      <td>134728</td>\n",
       "      <td>135533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1915 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      particleID    iid1    iid2    iid3    iid4    iid5    iid6    iid7  \\\n",
       "0     1000265260  134746  134710  135977  134622  134853  136039  136139   \n",
       "1     1001935289  135908  135628  134775  136331  134390  136306  134381   \n",
       "2     1002375244  136277  134791  136639  134624  134416  135315  135770   \n",
       "3     1002735288  136819  136242  134962  134606  136361  134390  134123   \n",
       "4     1002835245  135405  136751  134806  135863  136083  135390  136320   \n",
       "...          ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1910  1999075246  135716  134131  135146  135009  135268  135742  135805   \n",
       "1911  1999165241  134482  135136  135987  136178  136355  136646  136889   \n",
       "1912  1999345240  136293  135876  135329  136288  136356  135714  134205   \n",
       "1913  1999355239  134193  136200  135183  135391  135705  135853  135432   \n",
       "1914  1999735294  136898  134284  135317  135973  136439  136295  136588   \n",
       "\n",
       "        iid8    iid9  ...   iid91   iid92   iid93   iid94   iid95   iid96  \\\n",
       "0     136193  134762  ...  134315  134322  134510  135693  134997  135109   \n",
       "1     134409  134909  ...  136045  136458  136137  134266  134868  135007   \n",
       "2     136169  136530  ...  135374  135406  135453  135626  135924  135952   \n",
       "3     134231  134332  ...  136561  134322  136431  134502  134709  136243   \n",
       "4     136390  134853  ...  135534  135709  134206  136624  136931  136639   \n",
       "...      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "1910  136327  134737  ...  137028  135043  134556  134699  134729  134770   \n",
       "1911  136763  134782  ...  136791  135627  136258  134220  134356  136583   \n",
       "1912  134395  135003  ...  136679  136669  134933  136116  135566  136096   \n",
       "1913  135722  135813  ...  136327  134336  134523  136558  136988  134946   \n",
       "1914  137181  136322  ...  135127  136193  135885  135618  135641  136151   \n",
       "\n",
       "       iid97   iid98   iid99  iid100  \n",
       "0     135698  134633  136319  134726  \n",
       "1     135435  136062  136172  136179  \n",
       "2     136016  136106  136168  136189  \n",
       "3     134149  134162  134733  134788  \n",
       "4     136288  134634  134627  135606  \n",
       "...      ...     ...     ...     ...  \n",
       "1910  134923  135112  135368  135506  \n",
       "1911  136668  134291  136412  136326  \n",
       "1912  135390  135844  136285  134557  \n",
       "1913  135912  136476  134380  134397  \n",
       "1914  137029  134974  134728  135533  \n",
       "\n",
       "[1915 rows x 101 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a305e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
