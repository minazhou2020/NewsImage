{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cba1de",
   "metadata": {},
   "source": [
    "# Sample Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f2892",
   "metadata": {},
   "source": [
    "**This is a sample showing how to use following five classes**\\\n",
    "\\\n",
    "Data_Preprocessing\\\n",
    "URL_Matching\\\n",
    "Face_Name_Matching\\\n",
    "Model_Ensembling\\\n",
    "Image_Caption.py\\\n",
    "\\\n",
    "\\\n",
    "Requirements:\n",
    "Python 3.7\\\n",
    "\\\n",
    "Libraries:\n",
    "tensorflow\\\n",
    "pytorch\\\n",
    "py-googletrans\\\n",
    "NLTK\\\n",
    "gensim\\\n",
    "cv2\\\n",
    "icrawler\\\n",
    "DeepFace\\\n",
    "scipy\\\n",
    "matplotlib\\\n",
    "\\\n",
    "What you can do with this class:\n",
    "\n",
    "#### Data Preprocessing\n",
    "- combine the first two batches of files for training usage\n",
    "- the third batch used for validation\n",
    "- crawl training, valation, and test images from given URLs \n",
    "- extract features: image id, image url, article_title\n",
    "- tranlsate article title into English using Google Translate API (https://github.com/ssut/py-googletrans)\n",
    "\n",
    "#### URL Matching based Method\n",
    "- Feature Extraction: extract article url and image url from provided file\n",
    "- remove manually defined stop words from urls\n",
    "- URL tokenization\n",
    "- URL comparison: a pair of image url and article url is considered to be matched if they contains more than one common tokens.\n",
    "- sort the potential matched image list by the number of same tokens\n",
    "- Evaluate performance of this URL Matching based Method using MR100 on both training dataset and validation dataset\n",
    "\n",
    "#### Image Captioning based Model\n",
    "- acquired image caption from the pre-trained image captioning model (https://github.com/ruotianluo/ImageCaptioning.pytorch)\n",
    "- caculate the wmd between the each pair of image caption and article title\n",
    "- sort the potential matched image list by the wmd\n",
    "- Evaluate performance of this Image Captioning based Method using MR100 on both training dataset and validation dataset\n",
    "\n",
    "#### Face Matching\n",
    "##### Step 1:  Create a specific training dataset for face-name matching\n",
    "- A pair of article and image is used for this model training if the pair satisfies following two conditions: 1. the title of article include person's name, 2. the image is a human face image\n",
    "- Extract person's name from article title\n",
    "- Remove the image from this specific traning dataset if face can't be detected using multiple face detection frameworks\n",
    "- Build connections between the extracted name and the corresponding human face image\n",
    "- If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website.\n",
    "\n",
    "##### Step 2: Face Name Matching\n",
    "- Extract the person's names from testing article titles\n",
    "- Find the corresponding face images from the training dataset which created in step 1\n",
    "- Encode the face images into feature vectors\n",
    "- Compare the corresponding face images with each test face image by caculating the cosine distance between two feature vectors\n",
    "- Two face images are regared as matched if the cosine distance between two vectors is smaller or equal to 0.4 \n",
    "- Sort the potential matcheing image list by both cosine distance and total matches\n",
    "\n",
    "##### Step 3:Evaluation\n",
    "\n",
    "##### Step 4: Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e710e3",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- combine the first two batches of files for training usage\n",
    "- the third batch used for validation\n",
    "- crawl training, validation, and test images from given URLs \n",
    "- extract features: image id, image url, article_title\n",
    "- tranlsate article title into English using Google Translate API (https://github.com/ssut/py-googletrans) \\\n",
    "\n",
    "Note: The google translate python library only works in Linux OS. Save the translated text on your cloud/local machine for further processing in you use Window OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca68a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests  # to get image from the web\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import icrawler\n",
    "from icrawler.builtin  import GoogleImageCrawler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f641534",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=r\"../data\"\n",
    "os.path.isdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bf3f06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_01, train_02, validation, test = \"content2019-01-v3.tsv\", \"content2019-02-v3.tsv\", \\\n",
    "\"content2019-03-v3.tsv\", \"MediaEvalNewsImagesBatch04images.tsv\"\n",
    "processed_data_folder=r'processed_data/data'\n",
    "processed_img_folder=r'processed_data/img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e58e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    isExist = os.path.exists(path)\n",
    "\n",
    "    if not isExist:\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder('processed_data')\n",
    "create_folder('img')\n",
    "create_folder(r'processed_data/data')\n",
    "create_folder(r'processed_data/img')\n",
    "create_folder('result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ded37",
   "metadata": {},
   "source": [
    "### Combine Files\n",
    "We use the first two batches as train set, and the third as validation set\\\n",
    "we combine the fist two files into a whole file and combine all three files as train_eval file for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a729270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(filenames, output_file, skip):\n",
    "    \"\"\"\n",
    "    combine_csv combine a list of files into one file\n",
    "    :param filenames: a list of filename\n",
    "    :param output_file: output file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i in range(len(filenames)):\n",
    "            with open(filenames[i], \"r\", encoding=\"utf-8\") as infile:\n",
    "                if i !=0 and skip:\n",
    "                    next(infile)\n",
    "                contents = infile.read()\n",
    "                output.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_files([os.path.join(data_folder, train_01), os.path.join(data_folder, train_02)], \\\n",
    "              os.path.join(processed_data_folder, \"train.tsv\"))\n",
    "combine_files([os.path.join(data_folder, train_01), os.path.join(data_folder, train_02), \\\n",
    "                     os.path.join(data_folder, validation)], os.path.join(processed_data_folder, \"train_eval.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a5368ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_train=pd.read_csv(os.path.join(processed_data_folder, \"train.tsv\"), delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "053d859e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>aid</th>\n",
       "      <th>url</th>\n",
       "      <th>img</th>\n",
       "      <th>iid</th>\n",
       "      <th>hashvalue</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>nImpressions</th>\n",
       "      <th>nRecs</th>\n",
       "      <th>nClicks</th>\n",
       "      <th>imgFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509005634</td>\n",
       "      <td>118171</td>\n",
       "      <td>https://www.ksta.de/panorama/selbstgebauter-kn...</td>\n",
       "      <td>https://www.ksta.de/image/31811446/2x1/300/150...</td>\n",
       "      <td>118805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mann wird durch Böller schwer an der Hand verl...</td>\n",
       "      <td>In der Nähe von Celle ist ein 38-jähriger Mann...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31811446c816ac031a5a0b3add7d47a3813aec62rC.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>509006281</td>\n",
       "      <td>118172</td>\n",
       "      <td>https://www.ksta.de/wirtschaft/neuanfang-im-jo...</td>\n",
       "      <td>https://www.ksta.de/image/31808308/2x1/300/150...</td>\n",
       "      <td>118806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wie der Kölner Daniel Opoku sein altes Leben h...</td>\n",
       "      <td>Daniel Opoku, Outdoor-Kleidung, schlanke Statu...</td>\n",
       "      <td>1249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3180830885ee043d58cd779d33cca388888e6b57vZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>509006327</td>\n",
       "      <td>118173</td>\n",
       "      <td>https://www.ksta.de/wirtschaft/interview-ueber...</td>\n",
       "      <td>https://www.ksta.de/image/31808358/2x1/300/150...</td>\n",
       "      <td>118807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>„Wer auf Dauer im Beruf unglücklich ist, riski...</td>\n",
       "      <td>Herr Conen, gibt es so etwas wie ein „Navi“ fü...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31808358806096e3c65a13dcfd59cde588e7eb0auc.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>509006329</td>\n",
       "      <td>118174</td>\n",
       "      <td>https://www.ksta.de/panorama/panne-bei-feuerwe...</td>\n",
       "      <td>https://www.ksta.de/image/31811494/2x1/300/150...</td>\n",
       "      <td>118808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydneys Feuerwerks-Meister unterläuft peinlich...</td>\n",
       "      <td>Australiens Metropole Sydney hat das neue Jahr...</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31811494cf18c73a2fa30f16a949044c88b9beZx.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>509007066</td>\n",
       "      <td>118175</td>\n",
       "      <td>https://www.ksta.de/politik/us-sanktionen-kim-...</td>\n",
       "      <td>https://www.ksta.de/image/31811554/2x1/300/150...</td>\n",
       "      <td>118809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kim Jong Un droht mit Abkehr vom Annäherungskurs</td>\n",
       "      <td>Nordkoreas Machthaber Kim Jong Un droht im Str...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3181155429c50be85cf894763325d201ab241bc6oS.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5138</th>\n",
       "      <td>516107677</td>\n",
       "      <td>128265</td>\n",
       "      <td>https://www.ksta.de/wirtschaft/streit-zwischen...</td>\n",
       "      <td>https://www.ksta.de/image/32101778/2x1/300/150...</td>\n",
       "      <td>128906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Warum Produkte manchmal aus dem Laden verschwi...</td>\n",
       "      <td>Wer in den kommenden Wochen in den Regalen von...</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>32101778d7382c8de9174b2f14576f187590e33ji.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>516108139</td>\n",
       "      <td>128266</td>\n",
       "      <td>https://www.ksta.de/panorama/zu-viel-winterspe...</td>\n",
       "      <td>https://www.ksta.de/image/32102124/2x1/300/150...</td>\n",
       "      <td>128907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Helfer befreien hilflose Kanalratte aus Gullyd...</td>\n",
       "      <td>Große Rettungsaktion für eine kleine Ratte im ...</td>\n",
       "      <td>18</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>3210212424c32007453e8cb1655c1c6b42f6594eLA.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>516114886</td>\n",
       "      <td>128271</td>\n",
       "      <td>https://www.ksta.de/region/rhein-sieg-bonn/meh...</td>\n",
       "      <td>https://www.ksta.de/image/32102306/2x1/300/150...</td>\n",
       "      <td>128912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sonderticket und mehr Sicherheit im Zugverkehr</td>\n",
       "      <td>Der Zweckverband Nahverkehr Rheinland (NVR) un...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>32102306b354d659534ff9fb0d539ec6388cc9cDz.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5141</th>\n",
       "      <td>516119902</td>\n",
       "      <td>128274</td>\n",
       "      <td>https://www.ksta.de/politik/vergabe-von-projek...</td>\n",
       "      <td>https://www.ksta.de/image/30921850/2x1/300/150...</td>\n",
       "      <td>128915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NRW-Schulministerin Gebauer gerät unter Druck</td>\n",
       "      <td>NRW-Schulminister Yvonne Gebauer (FDP) steht w...</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>30921850ae48ccf720e1c102491ab2594acec02dtU.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>516122923</td>\n",
       "      <td>128276</td>\n",
       "      <td>https://www.ksta.de/koeln/koelsche-zeitangaben...</td>\n",
       "      <td>https://www.ksta.de/image/32102384/2x1/300/150...</td>\n",
       "      <td>128917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was „Noh Karneval“ und ein KVB-„Sofort“ mitein...</td>\n",
       "      <td>Kommst du als Imi in eine Stadt, in der man si...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3210238464abc6f99248c3c95a397061ee48daf4GJ.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5143 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article     aid                                                url  \\\n",
       "0     509005634  118171  https://www.ksta.de/panorama/selbstgebauter-kn...   \n",
       "1     509006281  118172  https://www.ksta.de/wirtschaft/neuanfang-im-jo...   \n",
       "2     509006327  118173  https://www.ksta.de/wirtschaft/interview-ueber...   \n",
       "3     509006329  118174  https://www.ksta.de/panorama/panne-bei-feuerwe...   \n",
       "4     509007066  118175  https://www.ksta.de/politik/us-sanktionen-kim-...   \n",
       "...         ...     ...                                                ...   \n",
       "5138  516107677  128265  https://www.ksta.de/wirtschaft/streit-zwischen...   \n",
       "5139  516108139  128266  https://www.ksta.de/panorama/zu-viel-winterspe...   \n",
       "5140  516114886  128271  https://www.ksta.de/region/rhein-sieg-bonn/meh...   \n",
       "5141  516119902  128274  https://www.ksta.de/politik/vergabe-von-projek...   \n",
       "5142  516122923  128276  https://www.ksta.de/koeln/koelsche-zeitangaben...   \n",
       "\n",
       "                                                    img     iid  hashvalue  \\\n",
       "0     https://www.ksta.de/image/31811446/2x1/300/150...  118805        NaN   \n",
       "1     https://www.ksta.de/image/31808308/2x1/300/150...  118806        NaN   \n",
       "2     https://www.ksta.de/image/31808358/2x1/300/150...  118807        NaN   \n",
       "3     https://www.ksta.de/image/31811494/2x1/300/150...  118808        NaN   \n",
       "4     https://www.ksta.de/image/31811554/2x1/300/150...  118809        NaN   \n",
       "...                                                 ...     ...        ...   \n",
       "5138  https://www.ksta.de/image/32101778/2x1/300/150...  128906        NaN   \n",
       "5139  https://www.ksta.de/image/32102124/2x1/300/150...  128907        NaN   \n",
       "5140  https://www.ksta.de/image/32102306/2x1/300/150...  128912        NaN   \n",
       "5141  https://www.ksta.de/image/30921850/2x1/300/150...  128915        NaN   \n",
       "5142  https://www.ksta.de/image/32102384/2x1/300/150...  128917        NaN   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Mann wird durch Böller schwer an der Hand verl...   \n",
       "1     Wie der Kölner Daniel Opoku sein altes Leben h...   \n",
       "2     „Wer auf Dauer im Beruf unglücklich ist, riski...   \n",
       "3     Sydneys Feuerwerks-Meister unterläuft peinlich...   \n",
       "4      Kim Jong Un droht mit Abkehr vom Annäherungskurs   \n",
       "...                                                 ...   \n",
       "5138  Warum Produkte manchmal aus dem Laden verschwi...   \n",
       "5139  Helfer befreien hilflose Kanalratte aus Gullyd...   \n",
       "5140     Sonderticket und mehr Sicherheit im Zugverkehr   \n",
       "5141      NRW-Schulministerin Gebauer gerät unter Druck   \n",
       "5142  Was „Noh Karneval“ und ein KVB-„Sofort“ mitein...   \n",
       "\n",
       "                                                   text  nImpressions  nRecs  \\\n",
       "0     In der Nähe von Celle ist ein 38-jähriger Mann...            14      0   \n",
       "1     Daniel Opoku, Outdoor-Kleidung, schlanke Statu...          1249      0   \n",
       "2     Herr Conen, gibt es so etwas wie ein „Navi“ fü...            36      0   \n",
       "3     Australiens Metropole Sydney hat das neue Jahr...            35      0   \n",
       "4     Nordkoreas Machthaber Kim Jong Un droht im Str...            17      0   \n",
       "...                                                 ...           ...    ...   \n",
       "5138  Wer in den kommenden Wochen in den Regalen von...             8     28   \n",
       "5139  Große Rettungsaktion für eine kleine Ratte im ...            18     43   \n",
       "5140  Der Zweckverband Nahverkehr Rheinland (NVR) un...             2      9   \n",
       "5141  NRW-Schulminister Yvonne Gebauer (FDP) steht w...            12      9   \n",
       "5142  Kommst du als Imi in eine Stadt, in der man si...             4      3   \n",
       "\n",
       "      nClicks                                         imgFile  \n",
       "0           0  31811446c816ac031a5a0b3add7d47a3813aec62rC.jpg  \n",
       "1           0  3180830885ee043d58cd779d33cca388888e6b57vZ.jpg  \n",
       "2           0  31808358806096e3c65a13dcfd59cde588e7eb0auc.jpg  \n",
       "3           0    31811494cf18c73a2fa30f16a949044c88b9beZx.jpg  \n",
       "4           0  3181155429c50be85cf894763325d201ab241bc6oS.jpg  \n",
       "...       ...                                             ...  \n",
       "5138        0   32101778d7382c8de9174b2f14576f187590e33ji.jpg  \n",
       "5139        0  3210212424c32007453e8cb1655c1c6b42f6594eLA.jpg  \n",
       "5140        0   32102306b354d659534ff9fb0d539ec6388cc9cDz.jpg  \n",
       "5141        0  30921850ae48ccf720e1c102491ab2594acec02dtU.jpg  \n",
       "5142        0  3210238464abc6f99248c3c95a397061ee48daf4GJ.jpg  \n",
       "\n",
       "[5143 rows x 12 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad09cce",
   "metadata": {},
   "source": [
    "### Load image from given urls to the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(data_file, img_folder, img_url_idx, img_id_idx):\n",
    "    \"\"\"\n",
    "    load_img download images from the url,\n",
    "    save images into the given image folder\n",
    "    and use image id as the image name\n",
    "    :param data_file: input file which include information such as img_url, img_id\n",
    "    :param img_folder: image folder where downloaded image are saved\n",
    "    :param img_url_idx: column idx of img url in the data_file\n",
    "    :param img_id_idx: column idx of img id in the data_file\n",
    "    \"\"\"\n",
    "    f = open(data_file, \"r\", encoding=\"utf-8\")\n",
    "    next(f)\n",
    "    print(\"start loading images\")\n",
    "    for line in f:\n",
    "        image_url = line.split(\"\\t\")[img_url_idx]\n",
    "        image_id = line.split(\"\\t\")[img_id_idx]\n",
    "        img_path = img_folder\n",
    "        isExist = os.path.exists(img_path)\n",
    "        if not isExist:\n",
    "            # Create a new directory because it does not exist\n",
    "            os.makedirs(img_path)\n",
    "            print(\"The image directory is created!\")\n",
    "        filename = os.path.join(img_path, image_id + \".jpg\")\n",
    "        r = requests.get(image_url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n",
    "        if r.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                r.raw.decode_content = True\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        else:\n",
    "            print(\"img can't be loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce119fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(os.path.join(processed_data_folder, \"train.tsv\"), \"img/training\", 3, 4)\n",
    "load_img(os.path.join(processed_data_folder, \"train_eval.tsv\"), \"img/train_eval\", 3, 4)\n",
    "load_img(os.path.join(data_folder, test), \"img/test\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97235b8f",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_data_file(input_file, new_file):\n",
    "    \"\"\"\n",
    "    reformat_data_file reformat the given input_file to facilate the further data processing\n",
    "    :param input_file: origin tsv file\n",
    "    :param new_file: output tsv file\n",
    "    \"\"\"\n",
    "    f = open(os.path.join(input_file), \"r\", encoding=\"utf-8\")\n",
    "    next(f)\n",
    "    with open(new_file, 'a', encoding=\"utf-8\") as the_file:\n",
    "        header = \"img_id\"+\"\\t\"+\"img_name\"+\"\\t\"+\"title\"\n",
    "        the_file.write(header + \"\\n\")\n",
    "        for line in f:\n",
    "            image_id = line.split(\"\\t\")[4] + \".jpg\"\n",
    "            image_url = line.split(\"\\t\")[3].split(\"/\")[-1]\n",
    "            title = line.split(\"\\t\")[6]\n",
    "            the_file.write(image_id + \"\\t\" +image_url + \"\\t\" + title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_data_file(os.path.join(processed_data_folder, \"train.tsv\"), os.path.join(processed_data_folder, \"train_title.tsv\"))\n",
    "reformat_data_file(os.path.join(processed_data_folder, \"train_eval.tsv\"), os.path.join(processed_data_folder, \"train_eval_title.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_data_file(os.path.join(data_folder, \"content2019-03-v3.tsv\"), os.path.join(processed_data_folder, \"eval_title.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f271e",
   "metadata": {},
   "source": [
    "### Tranlate Title\n",
    "translate article title into English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_trans(file_path):\n",
    "    file_text = open(file_path, 'r')\n",
    "    translator = Translator()\n",
    "    lines_text = file_text.readlines()\n",
    "    cnt = 0\n",
    "    trans_lines_text = []\n",
    "    for l_text in lines_text:\n",
    "        spes_text = l_text.split(\"\\t\")\n",
    "        result_text = translator.translate(spes_text[7], src='de')\n",
    "        trans_lines_text.append(result_text.text)\n",
    "        time.sleep(1)\n",
    "        cnt += 1\n",
    "        print(cnt)\n",
    "        if cnt % 50 == 0:\n",
    "            print(\"finish sub_lines_test_text \", cnt)\n",
    "    return trans_lines_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc23d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_title_eng(orig_file, titles_eng, output_file, aID_idx, title_idx):\n",
    "    lines = [line.strip() for line in open(orig_file, 'r', encoding=\"utf-8\")]\n",
    "    with open(output_file, 'a', encoding=\"utf-8\") as the_file:\n",
    "        for i in range(len(lines)):\n",
    "            title_eng = titles_eng[i].rstrip(\"\\n\")\n",
    "            segs = lines[i].strip(\"\\n\").split(\",\")\n",
    "            the_file.write(segs[aID_idx] + \"\\t\" + segs[title_idx] + '\\t' + title_eng + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893961d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_title(orig_file,output_file):\n",
    "    trans_lines_text=text_trans(orig_file)\n",
    "    comb_title_eng(orig_file, trans_lines_text, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f27acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_title(os.path.join(processed_data_folder, \"train_title.tsv\"), os.path.join(processed_data_folder,\"train_title_eng.tsv\"), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_title(os.path.join(processed_data_folder, \"eval_title.tsv\"), os.path.join(processed_data_folder,\"eval_title_eng.tsv\"), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3896cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_title(os.path.join(data_folder, \"MediaEvalNewsImagesBatch04articles.tsv\"), os.path.join(processed_data_folder,\"test_title_eng.tsv\"), 0 , 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679fb56",
   "metadata": {},
   "source": [
    "## URL Matching based Method\n",
    "- Feature Extraction: extract article url and image url from provided file\n",
    "- remove manually defined stop words from urls\n",
    "- URL tokenization\n",
    "- URL comparison: a pair of image url and article url is considered to be matched if they contains more than one common tokens. \n",
    "- sort the potential matched image list by the number of same tokens\n",
    "- Evaluate this performance of this method using MR100 on both training dataset and validation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cbe60",
   "metadata": {},
   "source": [
    "define stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b291f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['in', 'der', 'die', 'und', 'im', 'auf', 'mit', 'fuer', 'von', 'den', 'an', 'fc', 'das', 'am',\n",
    "                   'vor', 'aus', 'dem', 'anfang', 'sich', 'bei', 'ein', 'des', 'zu', 'sind', 'eine', 'ueber',\n",
    "                   'gegen', 'nach', 'ist', 'zum', 'beim', 'wird', 'nrw', 'nicht', 'als', 'mehr', 'ab', 'zur',\n",
    "                   'werden', 'hat', 's', 'wie', 'einem', 'auch', 'e', 'unter', 'wieder', 'vom', 'so', 'um',\n",
    "                   'noch', 'will', 'afd', 'war', 'strasse']\n",
    "test_img_file = \"MediaEvalNewsImagesBatch04images.tsv\"\n",
    "test_article_file = \"MediaEvalNewsImagesBatch04articles.tsv\"\n",
    "\n",
    "TR_A_ID_IDX = 1\n",
    "TR_I_ID_IDX = 4\n",
    "TR_IMG_URL_IDX = 3\n",
    "TR_TITLE_IDX = 2\n",
    "\n",
    "TEST_A_ID_IDX = 0\n",
    "TEST_I_ID_IDX = 1\n",
    "TEST_IMG_URL_IDX = 0\n",
    "TEST_TITLE_IDX = 2\n",
    "data_folder=r\"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae01a07",
   "metadata": {},
   "source": [
    "Extract the ground truth: the url paris in the training/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72fdfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gt(gt_file):\n",
    "    ground_truth = {}\n",
    "    with open(gt_file, encoding='utf-8') as file:\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        for line in lines:\n",
    "            segs = line.split(\"\\t\")\n",
    "            if len(segs) < 3:\n",
    "                break\n",
    "            ar_id = segs[TR_A_ID_IDX]\n",
    "            img_id = segs[TR_I_ID_IDX]\n",
    "            ground_truth[ar_id] = img_id\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd0a2c",
   "metadata": {},
   "source": [
    "Extract image url tokens from give files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b697cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_url_token(img_url_file, id_idx, img_url_idx):\n",
    "    img_id_name_dict = {}\n",
    "    with open(img_url_file, encoding='utf-8') as file:\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        for line in lines:\n",
    "            segs = line.split(\"\\t\")\n",
    "            if len(segs) < 3:\n",
    "                break\n",
    "            img_id = segs[id_idx]\n",
    "            img_name_full = segs[img_url_idx].split(\"/\")\n",
    "            img_name = img_name_full[len(img_name_full) - 1]\n",
    "            tokens = img_name.split(\".\")[0].split(\"-\")\n",
    "            tokens = [item for item in tokens if item.isalpha() and item != \"null\"]\n",
    "            img_id_name_dict[img_id] = tokens\n",
    "    return img_id_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e05fc3",
   "metadata": {},
   "source": [
    "Extract article url tokens from give files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6d6fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_token(article_file, a_id_idx, ar_name_idx):\n",
    "    art_id_name_dict = {}\n",
    "    with open(article_file, encoding=\"utf8\") as file:\n",
    "        next(file)\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        for line in lines:\n",
    "            segs = line.split(\"\\t\")\n",
    "            if len(segs) < 3:\n",
    "                break\n",
    "            ar_id = segs[a_id_idx]\n",
    "            ar_name_full = segs[ar_name_idx].split(\"/\")\n",
    "            ar_name = ar_name_full[len(ar_name_full) - 1]\n",
    "            tokens = ar_name.split(\".\")[0].split(\"-\")\n",
    "            tokens = [item for item in tokens if item.isalpha() and item != \"null\"]\n",
    "            art_id_name_dict[ar_id] = tokens\n",
    "        return art_id_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad7cd8",
   "metadata": {},
   "source": [
    "Find the matched URL paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c95b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_url(art_id_name_dict, img_id_name_dict):\n",
    "    print(\"matching url\")\n",
    "    candidates = {}\n",
    "    total = 0\n",
    "    result = {}\n",
    "    for art_k, art_v in art_id_name_dict.items():\n",
    "        cnt = 0\n",
    "        flag = False\n",
    "\n",
    "        for img_k, img_v in img_id_name_dict.items():\n",
    "            common_elements = [x for x in art_v if x in img_v and x not in stop_words and len(x) > 1]\n",
    "            if len(common_elements) > 0:\n",
    "                if art_k not in result:\n",
    "                    result[art_k] = []\n",
    "                result[art_k].append((img_k, len(common_elements)))\n",
    "                flag = True\n",
    "                cnt += 1\n",
    "                for ele in common_elements:\n",
    "                    if ele not in candidates:\n",
    "                        candidates[ele] = 0\n",
    "                    candidates[ele] += 1\n",
    "        if art_k in result:\n",
    "            temp_list = result[art_k]\n",
    "            temp_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            result[art_k] = [i[0] for i in temp_list]\n",
    "        if flag:\n",
    "            total += 1\n",
    "    print(total)\n",
    "    print(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d3f99",
   "metadata": {},
   "source": [
    "Write url matching results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a241c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_url_sim(result_file, result):\n",
    "    with open(result_file, 'w') as the_file:\n",
    "        for art_id, image_list in result.items():\n",
    "            line = art_id\n",
    "            for image in image_list:\n",
    "                line += \"\\t\" + image\n",
    "            the_file.write(line+\"\\n\")\n",
    "    the_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a27546",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "84954c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_file = \"MediaEvalNewsImagesBatch04articles.tsv\"\n",
    "test_img_file = \"MediaEvalNewsImagesBatch04articles.tsv\"\n",
    "def test(url_file, url_result_output):\n",
    "    img_id_name_dict = extract_img_url_token(os.path.join(data_folder, test_img_file),\n",
    "                                                  TEST_I_ID_IDX,\n",
    "                                                  TEST_IMG_URL_IDX)\n",
    "    article_id_name_dict = extract_article_token(os.path.join(data_folder, test_article_file),\n",
    "                                                      TEST_A_ID_IDX,\n",
    "                                                      TEST_TITLE_IDX)\n",
    "    result = match_url(article_id_name_dict, img_id_name_dict)\n",
    "    write_url_sim(url_result_output, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5f5fbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching url\n",
      "1772\n",
      "1772\n"
     ]
    }
   ],
   "source": [
    "url_file = r\"../data/MediaEvalNewsImagesBatch04articles.tsv\"\n",
    "url_result_output=r\"result/test_url_matching.tsv\"\n",
    "result=test(url_file,\"result/test_url_matching.tsv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592693d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69140e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result, ground_truth):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for ar_id, img_id in ground_truth.items():\n",
    "        if ar_id in result:\n",
    "            if img_id in result[ar_id][0:100]:\n",
    "                count += 1\n",
    "        total += 1\n",
    "    return count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4cfc2a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2387\n",
      "matching url\n",
      "2246\n",
      "2246\n",
      "MR100 in validation dataset is  0.4377880184331797\n"
     ]
    }
   ],
   "source": [
    "validation_file='content2019-03-v3.tsv'\n",
    "tr_file=os.path.join(data_folder, validation_file)\n",
    "ground_truth = extract_gt(tr_file)\n",
    "img_id_name_dict = extract_img_url_token(tr_file, TR_I_ID_IDX, TR_IMG_URL_IDX)\n",
    "article_id_name_dict = extract_article_token(tr_file, TR_A_ID_IDX, TR_TITLE_IDX)\n",
    "print(len(article_id_name_dict))\n",
    "result = match_url(article_id_name_dict, img_id_name_dict)\n",
    "write_url_sim(\"result/eval_url_matching.tsv\",result)\n",
    "evaluation_result = evaluate(result, ground_truth)\n",
    "print(\"MR100 in validation dataset is \", evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7dad25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5143\n",
      "matching url\n",
      "4958\n",
      "4958\n",
      "MR100 in training dataset is  0.40773867392572427\n"
     ]
    }
   ],
   "source": [
    "tr_file=r\"processed_data/data/train.tsv\"\n",
    "ground_truth = extract_gt(tr_file)\n",
    "img_id_name_dict = extract_img_url_token(tr_file, TR_I_ID_IDX, TR_IMG_URL_IDX)\n",
    "article_id_name_dict = extract_article_token(tr_file, TR_A_ID_IDX, TR_TITLE_IDX)\n",
    "print(len(article_id_name_dict))\n",
    "result = match_url(article_id_name_dict, img_id_name_dict)\n",
    "write_url_sim(\"result/tr_url_matching.tsv\",result)\n",
    "tr_result = evaluate(result, ground_truth)\n",
    "print(\"MR100 in training dataset is \", tr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f537d11",
   "metadata": {},
   "source": [
    "## Image Captioning based Model\n",
    "- acquired image caption from the pre-trained image captioning model (https://github.com/ruotianluo/ImageCaptioning.pytorch)\n",
    "- caculate the wmd between the each pair of image caption and article title\n",
    "- sort the potential matched image list by the wmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a42690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b3ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83ff14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption (caption_file):\n",
    "    articles_names=open(caption_file, 'r', encoding=\"utf-8\")\n",
    "    lines = [line.strip() for line in articles_names]\n",
    "    result_dict={}\n",
    "    for i in range(len(lines)):\n",
    "        orig_line=lines[i]\n",
    "        segs = orig_line.split(\"\\t\")\n",
    "        if segs[0] not in result_dict:\n",
    "            result_dict[segs[0]]=segs[1]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0764b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar_id_title (article_file, title_eng_idx):\n",
    "    articles_names=open(article_file, 'r', encoding=\"utf-8\")\n",
    "    next(articles_names)\n",
    "    lines = [line.strip() for line in articles_names]\n",
    "    result_dict={}\n",
    "    for i in range(len(lines)):\n",
    "        orig_line=lines[i]\n",
    "        segs = orig_line.split(\"\\t\")\n",
    "        if len(segs)>=3 and segs[0] not in result_dict:\n",
    "            result_dict[segs[0]]=segs[title_eng_idx]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea1c929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sim(id_title, caption_dict):\n",
    "    sim_result = {}\n",
    "    cnt = 0\n",
    "    for ar_id, title in id_title.items():\n",
    "        caption_sim=[]\n",
    "        for img_id, caption in caption_dict.items():\n",
    "            sim = model.wmdistance(title, caption)\n",
    "            caption_sim.append((img_id, sim))\n",
    "        cnt+=1\n",
    "        sim_result[ar_id]=caption_sim\n",
    "    return sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71527da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_wmd_sim(wmd_sim_file, sim_result):\n",
    "    f = open(wmd_sim_file, \"a\")\n",
    "    for key, v in sim_result.items():\n",
    "        for item in v:\n",
    "            result=key+\"\\t\"+os.path.basename(item[0])+\"\\t\"+str(item[1])+\"\\n\"\n",
    "            f.write(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c98fb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def img_cap_similarity(caption_file, title_file, wmd_sim_file, title_eng_idx):\n",
    "    caption_dict=get_caption (caption_file)\n",
    "    ar_id_title=get_ar_id_title (title_file, title_eng_idx)\n",
    "    sim_result=cal_sim(ar_id_title, caption_dict)\n",
    "    write_wmd_sim(wmd_sim_file, sim_result)\n",
    "    lines_caption_sim = []\n",
    "    with open(wmd_sim_file) as f:\n",
    "        lines_caption_sim = f.readlines()\n",
    "    ar_cap_sim_dic={}\n",
    "    for line in lines_caption_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_cap_sim_dic:\n",
    "            ar_cap_sim_dic[segs[0]]={}\n",
    "        ar_cap_sim_dic[segs[0]][segs[1]]= -float(segs[2])\n",
    "    sort_final_result=sort_dict(ar_cap_sim_dic)\n",
    "    return sort_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23c5b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(input):\n",
    "    result={}\n",
    "    for k, v in input.items():\n",
    "        sort_v=dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k]=sort_v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e00f1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cap_sim (sort_final_result):\n",
    "    count=0\n",
    "    for key, value in sort_final_result.items():\n",
    "        first_tuple_elements=[]\n",
    "        for a_tuple in value:\n",
    "            first_tuple_elements.append(a_tuple)\n",
    "        if key in first_tuple_elements[0:100]:\n",
    "            count+=1\n",
    "    return count/len(sort_final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b16608",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = img_cap_similarity(r\"processed_data\\data\\train_image_caption_result.txt\", \\\n",
    "                                  r\"processed_data\\data\\train_title_eng.tsv\",\\\n",
    "                                  r\"result/tr_caption_sim_wmd.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b950bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_final_result = img_cap_similarity(r\"processed_data\\data\\eval_image_caption_result.txt\", \\\n",
    "                                 \"processed_data\\data\\eval_title_eng.tsv\",\\\n",
    "                                \"result/eval_caption_sim_wmd.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "700008ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042312526183493925\n"
     ]
    }
   ],
   "source": [
    "print(eval_cap_sim (sort_final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = img_cap_similarity(r\"processed_data\\data\\test_image_caption_result.txt\", \\\n",
    "                                 \"processed_data\\data\\test_title_eng.tsv\",\\\n",
    "                                \"result/test_caption_sim_wmd.tsv\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a1038",
   "metadata": {},
   "source": [
    "## Face Matching\n",
    "### Step 1:  Create a specific training dataset for face-name matching\n",
    "- A pair of article and image is used for this model training if the pair satisfies following two conditions: 1. the title of article include person's name, 2. the image is a human face image\n",
    "- Extract person's name from article title\n",
    "- Remove the image from this specific traning dataset if face can't be detected using multiple face detection frameworks\n",
    "- Build connections between the extracted name and the corresponding human face image\n",
    "- If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website.\n",
    "### Step 2: Face Name Matching\n",
    "- Extract the person's names from testing article titles\n",
    "- Find the corresponding face images from the training dataset which created in step 1\n",
    "- Encode the face images into feature vectors\n",
    "- Compare the corresponding face images with each test face image by caculating the cosine distance between two feature vectors\n",
    "- Two face images are regared as matched if the cosine distance between two vectors is smaller or equal to 0.4 \n",
    "- Sort the potential matcheing image list by both cosine distance and total matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fae90",
   "metadata": {},
   "source": [
    "### Step 1:  Create a specific training dataset for face-name matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920d8f9",
   "metadata": {},
   "source": [
    "#### Name Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb20189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import icrawler\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import warnings\n",
    "from deepface import DeepFace\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = r\"C:\\Users\\yuxia\\Documents\\java-se-8u41-ri\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger(r'C:\\Users\\yuxia\\Downloads\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\classifiers\\english.all.3class.distsim.crf.ser.gz',\n",
    "                           r'C:\\Users\\yuxia\\Downloads\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\stanford-ner.jar',\n",
    "                           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_name(classified_text):\n",
    "    i=0\n",
    "    name_list=[]\n",
    "    while i < len(classified_text)-1:\n",
    "        if classified_text[i][1] == 'PERSON':\n",
    "            name = classified_text[i][0]\n",
    "            if classified_text[i+1][1]=='PERSON':\n",
    "                name+=\" \"+classified_text[i+1][0]\n",
    "                i+=1\n",
    "            name_list.append(name)\n",
    "        i+=1\n",
    "    if i == len(classified_text)-1 and classified_text[i][1] == 'PERSON':\n",
    "        name_list.append(classified_text[i][0])\n",
    "    return name_list\n",
    "\n",
    "\n",
    "def add_title_name(tr_file, output_file):\n",
    "    a_file = open(tr_file, encoding=\"utf8\")\n",
    "    next(a_file)\n",
    "    cnt=0\n",
    "    header=\"img_id\"+\"\\t\"+\"title\"+\"\\t\"+\"title_eng\"+\"\\t\"+\"title_names\"\n",
    "    with open(output_file, 'a',encoding=\"utf-8\") as the_file:\n",
    "        for line in a_file:\n",
    "            title_eng=line.split(\"\\t\")[2]\n",
    "            tokenized_text = word_tokenize(title_eng)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            names=concat_name(classified_text)\n",
    "            if len(names)>0:\n",
    "                names_str = ','.join(names)\n",
    "                print(names_str)\n",
    "                new_line=line.strip(\"\\n\")+\"\\t\"+names_str+\"\\n\"\n",
    "                cnt+=1\n",
    "            else:\n",
    "                new_line=line.strip(\"\\n\")+\"\\t \"+\"\\n\"\n",
    "            the_file.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_title_eng_file = r'processed_data\\data\\train_title_eng.tsv'\n",
    "eval_title_eng_file = r'processed_data\\data\\eval_title_eng.tsv'\n",
    "test_title_eng_file = r'processed_data\\data\\test_title_eng.tsv'\n",
    "tr_title_eng_name_file = r'processed_data\\data\\train_title_eng_name.tsv'\n",
    "eval_title_eng_name_file = r'processed_data\\data\\eval_title_eng_name.tsv'\n",
    "test_title_eng_name_file = r'processed_data\\data\\test_title_eng_name.tsv'\n",
    "add_title_name(tr_title_eng_file, output_file)\n",
    "add_title_name(eval_title_eng_file, output_file)\n",
    "add_title_name(test_title_eng_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=[r\"processed_data\\data\\train_title_eng_name.tsv\", r\"processed_data\\data\\eval_title_eng_name.tsv\"]\n",
    "output_file=r\"processed_data\\data\\train_eval_title_eng_name.tsv\"\n",
    "combine_files(filenames, output_file, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2f0ca",
   "metadata": {},
   "source": [
    "#### Processing the face image in the training/validation dataset\n",
    "- group images by the extracted names in the corresponding article\n",
    "- Created name indexing image dictionaries to fit unicode convention in OpenCV\n",
    "Note: OpenCV only accepts ASCII characters for image paths when reading and writing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7601828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_folder(name_file, train_face_folder, train_img_folder):\n",
    "    a_file = open(name_file, encoding=\"utf8\")\n",
    "    next(a_file)\n",
    "    for line in a_file:\n",
    "        line = line.strip(\"\\n\")\n",
    "        img_name = line.split(\"\\t\")[0]\n",
    "        names = line.split(\"\\t\")[4].rstrip()\n",
    "        if len(names) > 0:\n",
    "            path = os.path.join(train_face_folder, names.split(\",\")[0])\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            if os.path.exists(os.path.join(train_img_folder, img_name)):\n",
    "                copyfile(os.path.join(train_img_folder, img_name), os.path.join(path, img_name))\n",
    "\n",
    "def create_mapped_folder(d, mapped_folder):\n",
    "    if not os.path.isdir(mapped_folder):\n",
    "        os.mkdir(mapped_folder)\n",
    "    sub_directories = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    idx_name = {}\n",
    "    name_idx = {}\n",
    "    idx = 1\n",
    "    for sub_dir in sub_directories:\n",
    "        idx_name[sub_dir] = 'face_' + str(idx)\n",
    "        name_idx['face_' + str(idx)] = sub_dir\n",
    "        idx += 1\n",
    "    sub_full_paths = [os.path.join(d, o) for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    for sub_dir in sub_full_paths:\n",
    "         mapped_file_folder(sub_dir, mapped_folder, idx_name)\n",
    "    return idx_name, name_idx\n",
    "\n",
    "def mapped_file_folder(src, dest, idx_name):\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        dest_folder = os.path.join(dest, idx_name[os.path.basename(src)])\n",
    "        if not os.path.isdir(dest_folder):\n",
    "            os.mkdir(dest_folder)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_folder(r'processed_data\\data\\train_eval_title_eng_name.tsv', r\"processed_data\\img\\train_eval_faces\", r\"img\\train\")\n",
    "create_name_folder(r'processed_data\\data\\train_eval_title_eng_name.tsv', r\"processed_data\\img\\train_eval_faces\", r\"img\\eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_folder(r'processed_data\\data\\train_title_eng_name.tsv', r\"processed_data\\img\\train_faces\", r\"img\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eval_idx_name, tr_eval_name_idx = create_mapped_folder(r\"processed_data\\img\\train_eval_faces\", r\"processed_data\\img\\train_eval_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx_name, tr_name_idx = create_mapped_folder(r\"processed_data\\img\\train_faces\", r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d3c4c",
   "metadata": {},
   "source": [
    "- Remove non-face images from this specific training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_cv( file):\n",
    "    # Load the cascade\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    # Read the input image\n",
    "    img = cv2.imread(file)\n",
    "    # Convert into grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    width, height = gray.shape\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    # Draw rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        if w != width or height != h:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def deep_detect_backend(file):\n",
    "    backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface']\n",
    "    c = 0\n",
    "    for backend in backends:\n",
    "        try:\n",
    "            detected_face = DeepFace.detectFace(file, detector_backend=backend)\n",
    "        except:\n",
    "            c += 1\n",
    "    if c == len(backends):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def deep_detect(file):\n",
    "    models = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "    c = 0\n",
    "    for model in models:\n",
    "        try:\n",
    "            detected_face = DeepFace.detectFace(file, model_name=model)\n",
    "        except:\n",
    "            c += 1\n",
    "    if c == len(models):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_face_img(path):\n",
    "    face_img=[]\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    cnt=0\n",
    "    for sub_dir in sub_directories:\n",
    "        print(os.path.basename(sub_dir))\n",
    "        files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "        for file in files:\n",
    "            if deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file):\n",
    "                face_img.append(os.path.basename(file))\n",
    "            else:\n",
    "                os.remove(file)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_face_img_list=remove_no_face_img(r\"processed_data\\img\\train_eval_mapped_face\")\n",
    "train_face_img_list=remove_no_face_img(r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c059e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_folder(path):\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    for sub_dir in sub_directories:\n",
    "            files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "            if len(files)==0:\n",
    "                shutil.rmtree(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_empty_folder(r\"processed_data\\img\\train_eval_mapped_face\")\n",
    "remove_empty_folder(r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mapping(d):\n",
    "    sub_directories = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    idx_name = {}\n",
    "    name_idx = {}\n",
    "    idx = 1\n",
    "    for sub_dir in sub_directories:\n",
    "        idx_name[sub_dir] = 'face_' + str(idx)\n",
    "        name_idx['face_' + str(idx)] = sub_dir\n",
    "        idx += 1\n",
    "    return idx_name, name_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eval_idx_name, tr_eval_name_idx = find_mapping(r\"processed_data\\img\\train_eval_faces\")\n",
    "tr_idx_name, tr_name_idx = find_mapping(r\"processed_data\\img\\train_faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_eval_idx_name))\n",
    "print(len(tr_idx_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0e862",
   "metadata": {},
   "source": [
    "- Find the list of news articles with title including person's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar_name_list (article_file, title_eng_idx):\n",
    "    articles_names=open(article_file, 'r', encoding=\"utf-8\")\n",
    "    next(articles_names)\n",
    "    lines = [line.strip() for line in articles_names]\n",
    "    result=[]\n",
    "    for i in range(len(lines)):\n",
    "        orig_line=lines[i]\n",
    "        segs = orig_line.split(\"\\t\")\n",
    "        if len(segs) > title_eng_idx and len(segs[len(segs)-1].strip())>0 and segs[len(segs)-1].strip()!='NA':\n",
    "            result.append((segs[0], segs[title_eng_idx].split(\",\")[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4705cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ar_name_list=get_ar_name_list(r\"processed_data\\data\\train_title_eng_name.tsv\", 4)\n",
    "eval_ar_name_list=get_ar_name_list(r\"processed_data\\data\\eval_title_eng_name.tsv\", 4)\n",
    "test_ar_name_list=get_ar_name_list(r\"processed_data\\data\\test_title_eng_name.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ar_name_list))\n",
    "print(len(eval_ar_name_list))\n",
    "print(len(test_ar_name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305dc5c7",
   "metadata": {},
   "source": [
    "#### Face Image Crawling\n",
    "If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw_missing_images(ar_name_list, idx_name, train_mapped_face_path, crawl_face_path):\n",
    "    if not os.path.isdir(crawl_face_path):\n",
    "        os.mkdir(crawl_face_path)\n",
    "    for ar_name in ar_name_list:\n",
    "        if ar_name[1] in idx_name and ar_name[1] in idx_name and os.path.exists(os.path.join(train_mapped_face_path, idx_name[ar_name[1]])):\n",
    "                print(\"found\")\n",
    "        else:\n",
    "             \n",
    "            if not os.path.isdir(os.path.join(crawl_face_path, ar_name[1])):\n",
    "                os.mkdir(os.path.join(crawl_face_path, ar_name[1]))\n",
    "            google_crawler = GoogleImageCrawler(feeder_threads=1,parser_threads=2,downloader_threads=4,storage={'root_dir': os.path.join(crawl_face_path, ar_name[1])})\n",
    "            filters = dict(date=((2019, 1, 1), (2021, 7, 30)))\n",
    "            google_crawler.crawl(keyword=ar_name[1], filters=filters, max_num=5, file_idx_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ee8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw_missing_images(train_ar_name_list, {}, \"\",  r\"processed_data\\img\\crawl_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw_missing_images(eval_ar_name_list, tr_idx_name, r\"processed_data\\img\\train_mapped_face\", r\"processed_data\\img\\crawl_train_face\")\n",
    "craw_missing_images(test_ar_name_list, tr_eval_idx_name, r\"processed_data\\img\\train_eval_mapped_face\", r\"processed_data\\img\\crawl_train_eval_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_eval_idx_name, crawl_tr_eval_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_train_eval_face\", \\\n",
    "                                                                    r\"processed_data\\img\\crawl_train_eval_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_idx_name, crawl_tr_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_train_face\", \\\n",
    "                                                          r\"processed_data\\img\\crawl_train_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30974dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_idx_name, crawl_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_face\", \\\n",
    "                                                          r\"processed_data\\img\\crawl_face_mapped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1b2d6",
   "metadata": {},
   "source": [
    "- Remove non-face images from crawled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_face_img_crawl(path):\n",
    "    face_img=[]\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    cnt=0\n",
    "    for sub_dir in sub_directories:\n",
    "        print(os.path.basename(sub_dir))\n",
    "        files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "        for file in files:\n",
    "            if os.path.isfile(file) and (deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file)):\n",
    "                face_img.append(os.path.basename(file))\n",
    "            elif os.path.isdir (sub_dir):\n",
    "                shutil.rmtree(sub_dir)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd354f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_train_eval_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d57b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_train_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_eval_idx_name, crawl_tr_eval_name_idx = find_mapping(r\"processed_data\\img\\crawl_train_eval_face\")\n",
    "crawl_tr_idx_name, crawl_tr_name_idx = find_mapping(r\"processed_data\\img\\crawl_train_face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266d4b1",
   "metadata": {},
   "source": [
    "### Face Name Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e49ad4",
   "metadata": {},
   "source": [
    "#### Image Candidate Selection\n",
    "Only face images are selected to match with the images in training set created step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_face_image(src_path, dst_path):\n",
    "    if not os.path.isdir(dst_path):\n",
    "                os.mkdir(dst_path)\n",
    "    files = [os.path.join(src_path, f) for f in os.listdir(src_path) if f.endswith('.jpg')]\n",
    "    for file in files:\n",
    "        if deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file):\n",
    "            shutil.copy(file, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_face_image(r'img\\train', r'processed_data\\img\\train_face_candidate')\n",
    "select_face_image(r'img\\eval', r'processed_data\\img\\eval_face_candidate')\n",
    "select_face_image(r'img\\test', r'processed_data\\img\\test_face_candidate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac06e1",
   "metadata": {},
   "source": [
    "### caculate image similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "def get_face_similarity(face_img_candidate_dir, train_mapped_img_dir, ar_name_list, idx_name):\n",
    "    cnt = 0\n",
    "    record = 0\n",
    "    img_files = [f for f in os.listdir(face_img_candidate_dir) if f.endswith('.jpg')]\n",
    "    print(len(img_files))\n",
    "    ar_img_files = {}\n",
    "    for ar_name in ar_name_list:\n",
    "        if ar_name[1].strip() != 'NA' and ar_name[1] in idx_name:\n",
    "            img_db_path=\"\"\n",
    "            if os.path.exists(os.path.join(train_mapped_img_dir, idx_name[ar_name[1]])):\n",
    "                img_db_path=os.path.join(train_mapped_img_dir, idx_name[ar_name[1]])\n",
    "            if len(img_db_path)>0:\n",
    "                df_results = []\n",
    "                t = time.process_time()\n",
    "                count = 0\n",
    "                for img_file in img_files:\n",
    "                    img_path = os.path.join(face_img_candidate_dir, img_file)\n",
    "                    df = DeepFace.find(img_path=img_path, db_path=img_db_path,\n",
    "                                       model_name='Facenet', enforce_detection=False)\n",
    "                    if len(df) > 0:\n",
    "                        df_results.append((img_path, df['Facenet_cosine'].mean(), len(df)))\n",
    "                    else:\n",
    "                        df_results.append((img_path, \"NA\", 0))\n",
    "                    count += 1\n",
    "                ar_img_files[ar_name[0]] = df_results\n",
    "                cnt += 1\n",
    "                elapsed_time = time.process_time() - t\n",
    "\n",
    "                print(str(datetime.now()))\n",
    "                print(\"in \", elapsed_time, \"seconds complete\", cnt, \" name completed\", \" compared with \", count,\n",
    "                      \"images\")\n",
    "        record += 1\n",
    "        print(\"processing \", record, \" files\")\n",
    "    return ar_img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cbbb1",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_face_matching_similarity(output_file, ar_img_files):\n",
    "    f = open(output_file, \"a\")\n",
    "    for key, v in ar_img_files.items():\n",
    "        for item in v:\n",
    "            result = key + \"\\t\" + os.path.basename(item[0]) + \"\\t\" + str(item[1]) + \"\\t\" + str(item[2]) + \"\\n\"\n",
    "            f.write(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dictionary(input_dict):\n",
    "    result={}\n",
    "    for k, v in input_dict.items():\n",
    "        sort_v=dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k]=sort_v\n",
    "    return result\n",
    "\n",
    "def cal_face_matching_similarity(input_file):\n",
    "    image_train_sim = []\n",
    "    with open(input_file) as f:\n",
    "        image_train_sim = f.readlines()\n",
    "    ar_train_sim_dic={}\n",
    "    for line in image_train_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_train_sim_dic:\n",
    "            ar_train_sim_dic[segs[0]]=[]\n",
    "        ar_train_sim_dic[segs[0]].append((segs[1], segs[2], segs[3]))\n",
    "    ar_train_sim_dic_cal={}\n",
    "    for k, v in ar_train_sim_dic.items():\n",
    "        if k not in ar_train_sim_dic_cal:\n",
    "            ar_train_sim_dic_cal[k]={}\n",
    "        for item in v:\n",
    "            if int(item[2])==0:\n",
    "                sim=0\n",
    "            else:\n",
    "                sim=(1-float(item[1]))*int(item[2])\n",
    "            ar_train_sim_dic_cal[k][item[0]]= sim\n",
    "    return sort_dictionary(ar_train_sim_dic_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d43a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ar_img_files = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\train_mapped_face', eval_ar_name_list, tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e199bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/eval_face_similarity.tsv\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ar_img_files_crawl = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\crawl_train_face_mapped', eval_ar_name_list, crawl_tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/eval_face_similarity_crawl.tsv\",eval_ar_img_files_crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb5ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_face_matching = cal_face_matching_similarity(r\"result/eval_face_similarity.tsv\")\n",
    "eval_face_matching_crawl = cal_face_matching_similarity(r\"result/eval_face_similarity_crawl.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d012f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_MR(eval_face_matching):\n",
    "    count=0\n",
    "    for key, value in eval_face_matching.items():\n",
    "        first_tuple_elements=[]\n",
    "        for a_tuple in value:\n",
    "            first_tuple_elements.append(a_tuple)\n",
    "        if key in first_tuple_elements[0:100]:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb6dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21052631578947367\n",
      "0.0067085953878406705\n"
     ]
    }
   ],
   "source": [
    "print(cal_MR(eval_face_matching)/len(eval_face_matching))\n",
    "print(cal_MR(eval_face_matching)/2385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27fe665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10404624277456648\n",
      "0.007547169811320755\n"
     ]
    }
   ],
   "source": [
    "print(cal_MR(eval_face_matching_crawl)/len(eval_face_matching_crawl))\n",
    "print(cal_MR(eval_face_matching_crawl)/2385)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457d87a",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baedf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_img_files = get_face_similarity(r'processed_data\\img\\test_face_candidate',\n",
    "                                   r'processed_data\\img\\train_eval_mapped_face', \n",
    "                                        test_ar_name_list, \n",
    "                                        tr_eval_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/test_train_img_similarity.txt\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_img_files_crawl = get_face_similarity(r'processed_data\\img\\test_face_candidate',\n",
    "                                   r'processed_data\\img\\crawl_train_eval_face_mapped', \n",
    "                                              test_ar_name_list, \n",
    "                                              tr_eval_crawl_tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964606f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/test_crawl_img_similarity.txt\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc715e9",
   "metadata": {},
   "source": [
    "### Model Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f8a88",
   "metadata": {},
   "source": [
    "convert a file into a dictionary respresenting results from image captioning based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "692bbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_caption_result (caption_sim_wmd_file):\n",
    "    lines_caption_sim = []\n",
    "    with open(caption_sim_wmd_file) as f:\n",
    "        lines_caption_sim = f.readlines()\n",
    "    ar_cap_sim_dic={}\n",
    "    for line in lines_caption_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_cap_sim_dic:\n",
    "            ar_cap_sim_dic[segs[0]]={}\n",
    "        ar_cap_sim_dic[segs[0]][os.path.splitext(segs[1])[0]]= 1-float(segs[2])\n",
    "    return ar_cap_sim_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b802a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_cap_sim_dic=cal_caption_result(r\"result\\test_caption_sim_wmd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6a14046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n"
     ]
    }
   ],
   "source": [
    "print(len(ar_cap_sim_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a9d1b",
   "metadata": {},
   "source": [
    "convert a file into a dictionary respresenting results from face name matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c780c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_matching_result (face_matching_file):\n",
    "    image_train_sim = []\n",
    "    with open(face_matching_file) as f:\n",
    "        image_train_sim = f.readlines()\n",
    "    ar_train_sim_dic={}\n",
    "    for line in image_train_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_train_sim_dic:\n",
    "            ar_train_sim_dic[segs[0]]=[]\n",
    "        ar_train_sim_dic[segs[0]].append((os.path.splitext(segs[1])[0], segs[2], segs[3]))\n",
    "    ar_train_sim_dic_cal={}\n",
    "    \n",
    "    for k, v in ar_train_sim_dic.items():\n",
    "        if k not in ar_train_sim_dic_cal:\n",
    "            ar_train_sim_dic_cal[k]={}\n",
    "        for item in v:\n",
    "            if int(item[2])==0:\n",
    "                sim=0\n",
    "            else:\n",
    "                sim=(1-float(item[1]))*int(item[2])\n",
    "            ar_train_sim_dic_cal[k][item[0]]= sim\n",
    "    return ar_train_sim_dic_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4d2c8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_train_sim_dic_cal=face_matching_result(r\"result\\test_train_img_similarity.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "facafc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_crawl_sim_dic_cal=face_matching_result(r\"result\\test_crawl_img_similarity.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "65a4a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "print(len(ar_crawl_sim_dic_cal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146406a2",
   "metadata": {},
   "source": [
    "normalize value of a given dictionary (Min-max normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a0d9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dict(a_dict):\n",
    "    result={}\n",
    "    amin, amax = min(a_dict.values()), max(a_dict.values())\n",
    "    for k, v in a_dict.items():\n",
    "        if amax-amin==0:\n",
    "            result[k]=0\n",
    "        else:\n",
    "            result[k] = (v-amin) / (amax-amin)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d42278",
   "metadata": {},
   "source": [
    "normalize value of a given dictionary \n",
    "the dictionary respresents results from face name matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "76aafec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sim(a_dict):\n",
    "    result={}\n",
    "    print(len(a_dict))\n",
    "    for k, v in a_dict.items():\n",
    "        result[k]=norm_dict(v)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824293d6",
   "metadata": {},
   "source": [
    "sort the value in the given dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b06e99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(a_dict):\n",
    "    normalized_dict=norm_sim(a_dict)\n",
    "    result = {}\n",
    "    for k, v in normalized_dict.items():\n",
    "        sort_v = dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k] = sort_v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55846f5d",
   "metadata": {},
   "source": [
    "sort dictionary respresenting results from face name matching based model\n",
    "sort dictionary respresenting results from image captioning matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0392e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n",
      "128\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "cap_dict=sort_dict(ar_cap_sim_dic)\n",
    "train_dict=sort_dict(ar_train_sim_dic_cal)\n",
    "crawl_dict=sort_dict(ar_crawl_sim_dic_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba61773",
   "metadata": {},
   "source": [
    "merge results from image captioning based model and results from face matching based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fd62c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cap_face(cap_dict, train_dict, crawl_dict, weight_cap, weight_img):\n",
    "    result={}\n",
    "    for k, v in cap_dict.items():\n",
    "        img_id=os.path.splitext(k)[0]\n",
    "        result[img_id]=v\n",
    "        if k in train_dict:\n",
    "            for k_tr, v_tr in train_dict[k].items():\n",
    "                result[img_id][k_tr]=v_tr*weight_img\n",
    "        if k in crawl_dict:\n",
    "            for k_cr, v_cr in crawl_dict[k].items():\n",
    "                result[img_id][k_cr]=v_cr*weight_img\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1f74b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_face_result=merge_cap_face(cap_dict, train_dict, crawl_dict, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ba345398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\n"
     ]
    }
   ],
   "source": [
    "sorted_cap_face_result=sort_dict(cap_face_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37471b04",
   "metadata": {},
   "source": [
    "truncate image candidates into top 100 list for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "07169a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_result(a_dict):\n",
    "    for key, value in a_dict.items():\n",
    "        l = [*value]\n",
    "        l_100 = l[0:100]\n",
    "        a_dict[key]=l_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2238a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_result(sorted_cap_face_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "65694295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_cap_face_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86493cbb",
   "metadata": {},
   "source": [
    "acquire result from url matching based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a19be005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching url\n",
      "1772\n",
      "1772\n"
     ]
    }
   ],
   "source": [
    "img_id_name_dict = extract_img_url_token(\"../data/MediaEvalNewsImagesBatch04images.tsv\",\n",
    "                                              TEST_I_ID_IDX,\n",
    "                                              TEST_IMG_URL_IDX)\n",
    "article_id_name_dict = extract_article_token(\"../data/MediaEvalNewsImagesBatch04articles.tsv\", \n",
    "                                             TEST_A_ID_IDX,\n",
    "                                             TEST_TITLE_IDX)\n",
    "url_result = match_url(article_id_name_dict, img_id_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19dd42e",
   "metadata": {},
   "source": [
    "merge result from url matching based model into result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ff57c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_url(final_cap_face_result, url_result):\n",
    "    final_result={}\n",
    "    for s_id, s_value in final_cap_face_result.items():\n",
    "        if s_id in result:\n",
    "            diff_elements = [x for x in url_result[s_id] if x not in s_value ]\n",
    "            common_elements= [x for x in s_value if x in url_result[s_id] ]\n",
    "            tail_elements=s_value[len(s_value)-len(diff_elements):]\n",
    "            common_ele_in_tail=[x for x in common_elements if x in tail_elements]\n",
    "            if len(common_ele_in_tail)>0:\n",
    "                new_value=diff_elements+common_ele_in_tail+s_value[:len(s_value)-len(diff_elements)-len(common_ele_in_tail)]\n",
    "            else:\n",
    "                new_value=diff_elements+s_value[:len(s_value)-len(diff_elements)]\n",
    "            final_result[s_id]= new_value\n",
    "\n",
    "        else:\n",
    "            final_result[s_id]=s_value\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8a34c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=merge_url(sorted_cap_face_result, url_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069b19a",
   "metadata": {},
   "source": [
    "save final result into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb1a223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_result(output_file, final_result):\n",
    "    with open(output_file, \"w\") as the_file:\n",
    "        header=\"particleID\"\n",
    "        for i in range(100):\n",
    "            header+=\"\\t\"+\"iid\"+str(i+1)\n",
    "        the_file.write(header+\"\\n\")\n",
    "        for key, value in final_result.items():\n",
    "            the_file.write(key+'\\t'+ \"\\t\".join(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0be196e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_final_result(r\"result\\final_result.tsv\", final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f2c40",
   "metadata": {},
   "source": [
    "Result Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bc4e2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"result\\final_result.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f04f3b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>particleID</th>\n",
       "      <th>iid1</th>\n",
       "      <th>iid2</th>\n",
       "      <th>iid3</th>\n",
       "      <th>iid4</th>\n",
       "      <th>iid5</th>\n",
       "      <th>iid6</th>\n",
       "      <th>iid7</th>\n",
       "      <th>iid8</th>\n",
       "      <th>iid9</th>\n",
       "      <th>...</th>\n",
       "      <th>iid91</th>\n",
       "      <th>iid92</th>\n",
       "      <th>iid93</th>\n",
       "      <th>iid94</th>\n",
       "      <th>iid95</th>\n",
       "      <th>iid96</th>\n",
       "      <th>iid97</th>\n",
       "      <th>iid98</th>\n",
       "      <th>iid99</th>\n",
       "      <th>iid100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000265260</td>\n",
       "      <td>134746</td>\n",
       "      <td>134710</td>\n",
       "      <td>135977</td>\n",
       "      <td>134622</td>\n",
       "      <td>134853</td>\n",
       "      <td>136039</td>\n",
       "      <td>136139</td>\n",
       "      <td>136193</td>\n",
       "      <td>134762</td>\n",
       "      <td>...</td>\n",
       "      <td>134315</td>\n",
       "      <td>134322</td>\n",
       "      <td>134510</td>\n",
       "      <td>135693</td>\n",
       "      <td>134997</td>\n",
       "      <td>135109</td>\n",
       "      <td>135698</td>\n",
       "      <td>134633</td>\n",
       "      <td>136319</td>\n",
       "      <td>134726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001935289</td>\n",
       "      <td>135908</td>\n",
       "      <td>135628</td>\n",
       "      <td>134775</td>\n",
       "      <td>136331</td>\n",
       "      <td>134390</td>\n",
       "      <td>136306</td>\n",
       "      <td>134381</td>\n",
       "      <td>134409</td>\n",
       "      <td>134909</td>\n",
       "      <td>...</td>\n",
       "      <td>136045</td>\n",
       "      <td>136458</td>\n",
       "      <td>136137</td>\n",
       "      <td>134266</td>\n",
       "      <td>134868</td>\n",
       "      <td>135007</td>\n",
       "      <td>135435</td>\n",
       "      <td>136062</td>\n",
       "      <td>136172</td>\n",
       "      <td>136179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002375244</td>\n",
       "      <td>136277</td>\n",
       "      <td>134791</td>\n",
       "      <td>136639</td>\n",
       "      <td>134624</td>\n",
       "      <td>134416</td>\n",
       "      <td>135315</td>\n",
       "      <td>135770</td>\n",
       "      <td>136169</td>\n",
       "      <td>136530</td>\n",
       "      <td>...</td>\n",
       "      <td>135374</td>\n",
       "      <td>135406</td>\n",
       "      <td>135453</td>\n",
       "      <td>135626</td>\n",
       "      <td>135924</td>\n",
       "      <td>135952</td>\n",
       "      <td>136016</td>\n",
       "      <td>136106</td>\n",
       "      <td>136168</td>\n",
       "      <td>136189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002735288</td>\n",
       "      <td>136819</td>\n",
       "      <td>136242</td>\n",
       "      <td>134962</td>\n",
       "      <td>134606</td>\n",
       "      <td>136361</td>\n",
       "      <td>134390</td>\n",
       "      <td>134123</td>\n",
       "      <td>134231</td>\n",
       "      <td>134332</td>\n",
       "      <td>...</td>\n",
       "      <td>136561</td>\n",
       "      <td>134322</td>\n",
       "      <td>136431</td>\n",
       "      <td>134502</td>\n",
       "      <td>134709</td>\n",
       "      <td>136243</td>\n",
       "      <td>134149</td>\n",
       "      <td>134162</td>\n",
       "      <td>134733</td>\n",
       "      <td>134788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002835245</td>\n",
       "      <td>135405</td>\n",
       "      <td>136751</td>\n",
       "      <td>134806</td>\n",
       "      <td>135863</td>\n",
       "      <td>136083</td>\n",
       "      <td>135390</td>\n",
       "      <td>136320</td>\n",
       "      <td>136390</td>\n",
       "      <td>134853</td>\n",
       "      <td>...</td>\n",
       "      <td>135534</td>\n",
       "      <td>135709</td>\n",
       "      <td>134206</td>\n",
       "      <td>136624</td>\n",
       "      <td>136931</td>\n",
       "      <td>136639</td>\n",
       "      <td>136288</td>\n",
       "      <td>134634</td>\n",
       "      <td>134627</td>\n",
       "      <td>135606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>1999075246</td>\n",
       "      <td>135716</td>\n",
       "      <td>134131</td>\n",
       "      <td>135146</td>\n",
       "      <td>135009</td>\n",
       "      <td>135268</td>\n",
       "      <td>135742</td>\n",
       "      <td>135805</td>\n",
       "      <td>136327</td>\n",
       "      <td>134737</td>\n",
       "      <td>...</td>\n",
       "      <td>137028</td>\n",
       "      <td>135043</td>\n",
       "      <td>134556</td>\n",
       "      <td>134699</td>\n",
       "      <td>134729</td>\n",
       "      <td>134770</td>\n",
       "      <td>134923</td>\n",
       "      <td>135112</td>\n",
       "      <td>135368</td>\n",
       "      <td>135506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>1999165241</td>\n",
       "      <td>134482</td>\n",
       "      <td>135136</td>\n",
       "      <td>135987</td>\n",
       "      <td>136178</td>\n",
       "      <td>136355</td>\n",
       "      <td>136646</td>\n",
       "      <td>136889</td>\n",
       "      <td>136763</td>\n",
       "      <td>134782</td>\n",
       "      <td>...</td>\n",
       "      <td>136791</td>\n",
       "      <td>135627</td>\n",
       "      <td>136258</td>\n",
       "      <td>134220</td>\n",
       "      <td>134356</td>\n",
       "      <td>136583</td>\n",
       "      <td>136668</td>\n",
       "      <td>134291</td>\n",
       "      <td>136412</td>\n",
       "      <td>136326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>1999345240</td>\n",
       "      <td>136293</td>\n",
       "      <td>135876</td>\n",
       "      <td>135329</td>\n",
       "      <td>136288</td>\n",
       "      <td>136356</td>\n",
       "      <td>135714</td>\n",
       "      <td>134205</td>\n",
       "      <td>134395</td>\n",
       "      <td>135003</td>\n",
       "      <td>...</td>\n",
       "      <td>136679</td>\n",
       "      <td>136669</td>\n",
       "      <td>134933</td>\n",
       "      <td>136116</td>\n",
       "      <td>135566</td>\n",
       "      <td>136096</td>\n",
       "      <td>135390</td>\n",
       "      <td>135844</td>\n",
       "      <td>136285</td>\n",
       "      <td>134557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>1999355239</td>\n",
       "      <td>134193</td>\n",
       "      <td>136200</td>\n",
       "      <td>135183</td>\n",
       "      <td>135391</td>\n",
       "      <td>135705</td>\n",
       "      <td>135853</td>\n",
       "      <td>135432</td>\n",
       "      <td>135722</td>\n",
       "      <td>135813</td>\n",
       "      <td>...</td>\n",
       "      <td>136327</td>\n",
       "      <td>134336</td>\n",
       "      <td>134523</td>\n",
       "      <td>136558</td>\n",
       "      <td>136988</td>\n",
       "      <td>134946</td>\n",
       "      <td>135912</td>\n",
       "      <td>136476</td>\n",
       "      <td>134380</td>\n",
       "      <td>134397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>1999735294</td>\n",
       "      <td>136898</td>\n",
       "      <td>134284</td>\n",
       "      <td>135317</td>\n",
       "      <td>135973</td>\n",
       "      <td>136439</td>\n",
       "      <td>136295</td>\n",
       "      <td>136588</td>\n",
       "      <td>137181</td>\n",
       "      <td>136322</td>\n",
       "      <td>...</td>\n",
       "      <td>135127</td>\n",
       "      <td>136193</td>\n",
       "      <td>135885</td>\n",
       "      <td>135618</td>\n",
       "      <td>135641</td>\n",
       "      <td>136151</td>\n",
       "      <td>137029</td>\n",
       "      <td>134974</td>\n",
       "      <td>134728</td>\n",
       "      <td>135533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1915 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      particleID    iid1    iid2    iid3    iid4    iid5    iid6    iid7  \\\n",
       "0     1000265260  134746  134710  135977  134622  134853  136039  136139   \n",
       "1     1001935289  135908  135628  134775  136331  134390  136306  134381   \n",
       "2     1002375244  136277  134791  136639  134624  134416  135315  135770   \n",
       "3     1002735288  136819  136242  134962  134606  136361  134390  134123   \n",
       "4     1002835245  135405  136751  134806  135863  136083  135390  136320   \n",
       "...          ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1910  1999075246  135716  134131  135146  135009  135268  135742  135805   \n",
       "1911  1999165241  134482  135136  135987  136178  136355  136646  136889   \n",
       "1912  1999345240  136293  135876  135329  136288  136356  135714  134205   \n",
       "1913  1999355239  134193  136200  135183  135391  135705  135853  135432   \n",
       "1914  1999735294  136898  134284  135317  135973  136439  136295  136588   \n",
       "\n",
       "        iid8    iid9  ...   iid91   iid92   iid93   iid94   iid95   iid96  \\\n",
       "0     136193  134762  ...  134315  134322  134510  135693  134997  135109   \n",
       "1     134409  134909  ...  136045  136458  136137  134266  134868  135007   \n",
       "2     136169  136530  ...  135374  135406  135453  135626  135924  135952   \n",
       "3     134231  134332  ...  136561  134322  136431  134502  134709  136243   \n",
       "4     136390  134853  ...  135534  135709  134206  136624  136931  136639   \n",
       "...      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "1910  136327  134737  ...  137028  135043  134556  134699  134729  134770   \n",
       "1911  136763  134782  ...  136791  135627  136258  134220  134356  136583   \n",
       "1912  134395  135003  ...  136679  136669  134933  136116  135566  136096   \n",
       "1913  135722  135813  ...  136327  134336  134523  136558  136988  134946   \n",
       "1914  137181  136322  ...  135127  136193  135885  135618  135641  136151   \n",
       "\n",
       "       iid97   iid98   iid99  iid100  \n",
       "0     135698  134633  136319  134726  \n",
       "1     135435  136062  136172  136179  \n",
       "2     136016  136106  136168  136189  \n",
       "3     134149  134162  134733  134788  \n",
       "4     136288  134634  134627  135606  \n",
       "...      ...     ...     ...     ...  \n",
       "1910  134923  135112  135368  135506  \n",
       "1911  136668  134291  136412  136326  \n",
       "1912  135390  135844  136285  134557  \n",
       "1913  135912  136476  134380  134397  \n",
       "1914  137029  134974  134728  135533  \n",
       "\n",
       "[1915 rows x 101 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a305e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
