{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f75bb4",
   "metadata": {},
   "source": [
    "**This is a sample of how to use Face_Matching class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a1038",
   "metadata": {},
   "source": [
    "## Face Matching\n",
    "### Step 1:  Create a specific training dataset for face-name matching\n",
    "- A pair of article and image is used for this model training if the pair satisfies following two conditions: 1. the title of article include person's name, 2. the image is a human face image\n",
    "- Extract person's name from article title\n",
    "- Remove the image from this specific traning dataset if face can't be detected using multiple face detection frameworks\n",
    "- Build connections between the extracted name and the corresponding human face image\n",
    "- If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website.\n",
    "### Step 2: Face Name Matching\n",
    "- Extract the person's names from testing article titles\n",
    "- Find the corresponding face images from the training dataset which created in step 1\n",
    "- Encode the face images into feature vectors\n",
    "- Compare the corresponding face images with each test face image by caculating the cosine distance between two feature vectors\n",
    "- Two face images are regared as matched if the cosine distance between two vectors is smaller or equal to 0.4 \n",
    "- Sort the potential matcheing image list by both cosine distance and total matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fae90",
   "metadata": {},
   "source": [
    "### Step 1:  Create a specific training dataset for face-name matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc753c0",
   "metadata": {},
   "source": [
    "#### Name Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb20189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import icrawler\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import warnings\n",
    "from deepface import DeepFace\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = r\"C:\\Users\\yuxia\\Documents\\java-se-8u41-ri\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger(r'C:\\Users\\yuxia\\Downloads\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\classifiers\\english.all.3class.distsim.crf.ser.gz',\n",
    "                           r'C:\\Users\\yuxia\\Downloads\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\stanford-ner.jar',\n",
    "                           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_name(classified_text):\n",
    "    i=0\n",
    "    name_list=[]\n",
    "    while i < len(classified_text)-1:\n",
    "        if classified_text[i][1] == 'PERSON':\n",
    "            name = classified_text[i][0]\n",
    "            if classified_text[i+1][1]=='PERSON':\n",
    "                name+=\" \"+classified_text[i+1][0]\n",
    "                i+=1\n",
    "            name_list.append(name)\n",
    "        i+=1\n",
    "    if i == len(classified_text)-1 and classified_text[i][1] == 'PERSON':\n",
    "        name_list.append(classified_text[i][0])\n",
    "    return name_list\n",
    "\n",
    "\n",
    "def add_title_name(tr_file, output_file):\n",
    "    a_file = open(tr_file, encoding=\"utf8\")\n",
    "    next(a_file)\n",
    "    cnt=0\n",
    "    header=\"img_id\"+\"\\t\"+\"title\"+\"\\t\"+\"title_eng\"+\"\\t\"+\"title_names\"\n",
    "    with open(output_file, 'a',encoding=\"utf-8\") as the_file:\n",
    "        for line in a_file:\n",
    "            title_eng=line.split(\"\\t\")[2]\n",
    "            tokenized_text = word_tokenize(title_eng)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            names=concat_name(classified_text)\n",
    "            if len(names)>0:\n",
    "                names_str = ','.join(names)\n",
    "                print(names_str)\n",
    "                new_line=line.strip(\"\\n\")+\"\\t\"+names_str+\"\\n\"\n",
    "                cnt+=1\n",
    "            else:\n",
    "                new_line=line.strip(\"\\n\")+\"\\t \"+\"\\n\"\n",
    "            the_file.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_title_eng_file = r'processed_data\\data\\train_title_eng.tsv'\n",
    "eval_title_eng_file = r'processed_data\\data\\eval_title_eng.tsv'\n",
    "test_title_eng_file = r'processed_data\\data\\test_title_eng.tsv'\n",
    "tr_title_eng_name_file = r'processed_data\\data\\train_title_eng_name.tsv'\n",
    "eval_title_eng_name_file = r'processed_data\\data\\eval_title_eng_name.tsv'\n",
    "test_title_eng_name_file = r'processed_data\\data\\test_title_eng_name.tsv'\n",
    "add_title_name(tr_title_eng_file, output_file)\n",
    "add_title_name(eval_title_eng_file, output_file)\n",
    "add_title_name(test_title_eng_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=[r\"processed_data\\data\\train_title_eng_name.tsv\", r\"processed_data\\data\\eval_title_eng_name.tsv\"]\n",
    "output_file=r\"processed_data\\data\\train_eval_title_eng_name.tsv\"\n",
    "combine_files(filenames, output_file, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f2cbb",
   "metadata": {},
   "source": [
    "#### Processing the face image in the training/validation dataset\n",
    "- group images by the extracted names in the corresponding article\n",
    "- Created name indexing image dictionaries to fit unicode convention in OpenCV\n",
    "Note: OpenCV only accepts ASCII characters for image paths when reading and writing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7601828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_folder(name_file, train_face_folder, train_img_folder):\n",
    "    a_file = open(name_file, encoding=\"utf8\")\n",
    "    next(a_file)\n",
    "    for line in a_file:\n",
    "        line = line.strip(\"\\n\")\n",
    "        img_name = line.split(\"\\t\")[0]\n",
    "        names = line.split(\"\\t\")[4].rstrip()\n",
    "        if len(names) > 0:\n",
    "            path = os.path.join(train_face_folder, names.split(\",\")[0])\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            if os.path.exists(os.path.join(train_img_folder, img_name)):\n",
    "                copyfile(os.path.join(train_img_folder, img_name), os.path.join(path, img_name))\n",
    "\n",
    "def create_mapped_folder(d, mapped_folder):\n",
    "    if not os.path.isdir(mapped_folder):\n",
    "        os.mkdir(mapped_folder)\n",
    "    sub_directories = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    idx_name = {}\n",
    "    name_idx = {}\n",
    "    idx = 1\n",
    "    for sub_dir in sub_directories:\n",
    "        idx_name[sub_dir] = 'face_' + str(idx)\n",
    "        name_idx['face_' + str(idx)] = sub_dir\n",
    "        idx += 1\n",
    "    sub_full_paths = [os.path.join(d, o) for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    for sub_dir in sub_full_paths:\n",
    "         mapped_file_folder(sub_dir, mapped_folder, idx_name)\n",
    "    return idx_name, name_idx\n",
    "\n",
    "def mapped_file_folder(src, dest, idx_name):\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        dest_folder = os.path.join(dest, idx_name[os.path.basename(src)])\n",
    "        if not os.path.isdir(dest_folder):\n",
    "            os.mkdir(dest_folder)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_folder(r'processed_data\\data\\train_eval_title_eng_name.tsv', r\"processed_data\\img\\train_eval_faces\", r\"img\\train\")\n",
    "create_name_folder(r'processed_data\\data\\train_eval_title_eng_name.tsv', r\"processed_data\\img\\train_eval_faces\", r\"img\\eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_folder(r'processed_data\\data\\train_title_eng_name.tsv', r\"processed_data\\img\\train_faces\", r\"img\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eval_idx_name, tr_eval_name_idx = create_mapped_folder(r\"processed_data\\img\\train_eval_faces\", r\"processed_data\\img\\train_eval_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx_name, tr_name_idx = create_mapped_folder(r\"processed_data\\img\\train_faces\", r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9f59e",
   "metadata": {},
   "source": [
    "- Remove non-face images from this specific training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_cv( file):\n",
    "    # Load the cascade\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    # Read the input image\n",
    "    img = cv2.imread(file)\n",
    "    # Convert into grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    width, height = gray.shape\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    # Draw rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        if w != width or height != h:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def deep_detect_backend(file):\n",
    "    backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface']\n",
    "    c = 0\n",
    "    for backend in backends:\n",
    "        try:\n",
    "            detected_face = DeepFace.detectFace(file, detector_backend=backend)\n",
    "        except:\n",
    "            c += 1\n",
    "    if c == len(backends):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def deep_detect(file):\n",
    "    models = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "    c = 0\n",
    "    for model in models:\n",
    "        try:\n",
    "            detected_face = DeepFace.detectFace(file, model_name=model)\n",
    "        except:\n",
    "            c += 1\n",
    "    if c == len(models):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_face_img(path):\n",
    "    face_img=[]\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    cnt=0\n",
    "    for sub_dir in sub_directories:\n",
    "        print(os.path.basename(sub_dir))\n",
    "        files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "        for file in files:\n",
    "            if deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file):\n",
    "                face_img.append(os.path.basename(file))\n",
    "            else:\n",
    "                os.remove(file)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_face_img_list=remove_no_face_img(r\"processed_data\\img\\train_eval_mapped_face\")\n",
    "train_face_img_list=remove_no_face_img(r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c059e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_folder(path):\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    for sub_dir in sub_directories:\n",
    "            files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "            if len(files)==0:\n",
    "                shutil.rmtree(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_empty_folder(r\"processed_data\\img\\train_eval_mapped_face\")\n",
    "remove_empty_folder(r\"processed_data\\img\\train_mapped_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mapping(d):\n",
    "    sub_directories = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d, o))]\n",
    "    idx_name = {}\n",
    "    name_idx = {}\n",
    "    idx = 1\n",
    "    for sub_dir in sub_directories:\n",
    "        idx_name[sub_dir] = 'face_' + str(idx)\n",
    "        name_idx['face_' + str(idx)] = sub_dir\n",
    "        idx += 1\n",
    "    return idx_name, name_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_eval_idx_name, tr_eval_name_idx = find_mapping(r\"processed_data\\img\\train_eval_faces\")\n",
    "tr_idx_name, tr_name_idx = find_mapping(r\"processed_data\\img\\train_faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_eval_idx_name))\n",
    "print(len(tr_idx_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2d13a",
   "metadata": {},
   "source": [
    "- Find the list of news articles with title including person's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar_name_list (article_file, title_eng_idx):\n",
    "    articles_names=open(article_file, 'r', encoding=\"utf-8\")\n",
    "    next(articles_names)\n",
    "    lines = [line.strip() for line in articles_names]\n",
    "    result=[]\n",
    "    for i in range(len(lines)):\n",
    "        orig_line=lines[i]\n",
    "        segs = orig_line.split(\"\\t\")\n",
    "        if len(segs) > title_eng_idx and len(segs[len(segs)-1].strip())>0 and segs[len(segs)-1].strip()!='NA':\n",
    "            result.append((segs[0], segs[title_eng_idx].split(\",\")[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4705cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ar_name_list=get_ar_name_list(r\"processed_data\\data\\train_title_eng_name.tsv\", 4)\n",
    "eval_ar_name_list=get_ar_name_list(r\"processed_data\\data\\eval_title_eng_name.tsv\", 4)\n",
    "test_ar_name_list=get_ar_name_list(r\"processed_data\\data\\test_title_eng_name.tsv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ar_name_list))\n",
    "print(len(eval_ar_name_list))\n",
    "print(len(test_ar_name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc7fc5",
   "metadata": {},
   "source": [
    "#### Face Image Crawling\n",
    "If there is no connected image for the extracted name in the training dataset, we crawl five face image using the extracted name as keyword from website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw_missing_images(ar_name_list, idx_name, train_mapped_face_path, crawl_face_path):\n",
    "    if not os.path.isdir(crawl_face_path):\n",
    "        os.mkdir(crawl_face_path)\n",
    "    for ar_name in ar_name_list:\n",
    "        if ar_name[1] in idx_name and ar_name[1] in idx_name and os.path.exists(os.path.join(train_mapped_face_path, idx_name[ar_name[1]])):\n",
    "                print(\"found\")\n",
    "        else:\n",
    "             \n",
    "            if not os.path.isdir(os.path.join(crawl_face_path, ar_name[1])):\n",
    "                os.mkdir(os.path.join(crawl_face_path, ar_name[1]))\n",
    "            google_crawler = GoogleImageCrawler(feeder_threads=1,parser_threads=2,downloader_threads=4,storage={'root_dir': os.path.join(crawl_face_path, ar_name[1])})\n",
    "            filters = dict(date=((2019, 1, 1), (2021, 7, 30)))\n",
    "            google_crawler.crawl(keyword=ar_name[1], filters=filters, max_num=5, file_idx_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ee8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw_missing_images(train_ar_name_list, {}, \"\",  r\"processed_data\\img\\crawl_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw_missing_images(eval_ar_name_list, tr_idx_name, r\"processed_data\\img\\train_mapped_face\", r\"processed_data\\img\\crawl_train_face\")\n",
    "craw_missing_images(test_ar_name_list, tr_eval_idx_name, r\"processed_data\\img\\train_eval_mapped_face\", r\"processed_data\\img\\crawl_train_eval_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_eval_idx_name, crawl_tr_eval_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_train_eval_face\", \\\n",
    "                                                                    r\"processed_data\\img\\crawl_train_eval_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_idx_name, crawl_tr_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_train_face\", \\\n",
    "                                                          r\"processed_data\\img\\crawl_train_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30974dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_idx_name, crawl_name_idx=create_mapped_folder(r\"processed_data\\img\\crawl_face\", \\\n",
    "                                                          r\"processed_data\\img\\crawl_face_mapped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff3357",
   "metadata": {},
   "source": [
    "- Remove non-face images from crawled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_face_img_crawl(path):\n",
    "    face_img=[]\n",
    "    sub_directories = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    cnt=0\n",
    "    for sub_dir in sub_directories:\n",
    "        print(os.path.basename(sub_dir))\n",
    "        files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('.jpg')]\n",
    "        for file in files:\n",
    "            if os.path.isfile(file) and (deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file)):\n",
    "                face_img.append(os.path.basename(file))\n",
    "            elif os.path.isdir (sub_dir):\n",
    "                shutil.rmtree(sub_dir)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd354f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_train_eval_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d57b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_train_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_no_face_img_crawl(r\"processed_data\\img\\crawl_face_mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tr_eval_idx_name, crawl_tr_eval_name_idx = find_mapping(r\"processed_data\\img\\crawl_train_eval_face\")\n",
    "crawl_tr_idx_name, crawl_tr_name_idx = find_mapping(r\"processed_data\\img\\crawl_train_face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06547d7c",
   "metadata": {},
   "source": [
    "### Face Name Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3d812",
   "metadata": {},
   "source": [
    "#### Image Candidate Selection\n",
    "Only face images are selected to match with the images in training set created step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ebc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_face_image(src_path, dst_path):\n",
    "    if not os.path.isdir(dst_path):\n",
    "                os.mkdir(dst_path)\n",
    "    files = [os.path.join(src_path, f) for f in os.listdir(src_path) if f.endswith('.jpg')]\n",
    "    for file in files:\n",
    "        if deep_detect(file) or deep_detect_backend(file) or detect_face_cv(file):\n",
    "            shutil.copy(file, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_face_image(r'img\\train', r'processed_data\\img\\train_face_candidate')\n",
    "select_face_image(r'img\\eval', r'processed_data\\img\\eval_face_candidate')\n",
    "select_face_image(r'img\\test', r'processed_data\\img\\test_face_candidate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524477b5",
   "metadata": {},
   "source": [
    "### caculate image similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "def get_face_similarity(face_img_candidate_dir, train_mapped_img_dir, ar_name_list, idx_name):\n",
    "    cnt = 0\n",
    "    record = 0\n",
    "    img_files = [f for f in os.listdir(face_img_candidate_dir) if f.endswith('.jpg')]\n",
    "    print(len(img_files))\n",
    "    ar_img_files = {}\n",
    "    for ar_name in ar_name_list:\n",
    "        if ar_name[1].strip() != 'NA' and ar_name[1] in idx_name:\n",
    "            img_db_path=\"\"\n",
    "            if os.path.exists(os.path.join(train_mapped_img_dir, idx_name[ar_name[1]])):\n",
    "                img_db_path=os.path.join(train_mapped_img_dir, idx_name[ar_name[1]])\n",
    "            if len(img_db_path)>0:\n",
    "                df_results = []\n",
    "                t = time.process_time()\n",
    "                count = 0\n",
    "                for img_file in img_files:\n",
    "                    img_path = os.path.join(face_img_candidate_dir, img_file)\n",
    "                    df = DeepFace.find(img_path=img_path, db_path=img_db_path,\n",
    "                                       model_name='Facenet', enforce_detection=False)\n",
    "                    if len(df) > 0:\n",
    "                        df_results.append((img_path, df['Facenet_cosine'].mean(), len(df)))\n",
    "                    else:\n",
    "                        df_results.append((img_path, \"NA\", 0))\n",
    "                    count += 1\n",
    "                ar_img_files[ar_name[0]] = df_results\n",
    "                cnt += 1\n",
    "                elapsed_time = time.process_time() - t\n",
    "\n",
    "                print(str(datetime.now()))\n",
    "                print(\"in \", elapsed_time, \"seconds complete\", cnt, \" name completed\", \" compared with \", count,\n",
    "                      \"images\")\n",
    "        record += 1\n",
    "        print(\"processing \", record, \" files\")\n",
    "    return ar_img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145c5f3",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_face_matching_similarity(output_file, ar_img_files):\n",
    "    f = open(output_file, \"a\")\n",
    "    for key, v in ar_img_files.items():\n",
    "        for item in v:\n",
    "            result = key + \"\\t\" + os.path.basename(item[0]) + \"\\t\" + str(item[1]) + \"\\t\" + str(item[2]) + \"\\n\"\n",
    "            f.write(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dictionary(input_dict):\n",
    "    result={}\n",
    "    for k, v in input_dict.items():\n",
    "        sort_v=dict(sorted(v.items(), key=lambda item: item[1], reverse=True))\n",
    "        result[k]=sort_v\n",
    "    return result\n",
    "\n",
    "def cal_face_matching_similarity(input_file):\n",
    "    image_train_sim = []\n",
    "    with open(input_file) as f:\n",
    "        image_train_sim = f.readlines()\n",
    "    ar_train_sim_dic={}\n",
    "    for line in image_train_sim:\n",
    "        segs=line.strip().split(\"\\t\")\n",
    "        if segs[0] not in ar_train_sim_dic:\n",
    "            ar_train_sim_dic[segs[0]]=[]\n",
    "        ar_train_sim_dic[segs[0]].append((segs[1], segs[2], segs[3]))\n",
    "    ar_train_sim_dic_cal={}\n",
    "    for k, v in ar_train_sim_dic.items():\n",
    "        if k not in ar_train_sim_dic_cal:\n",
    "            ar_train_sim_dic_cal[k]={}\n",
    "        for item in v:\n",
    "            if int(item[2])==0:\n",
    "                sim=0\n",
    "            else:\n",
    "                sim=(1-float(item[1]))*int(item[2])\n",
    "            ar_train_sim_dic_cal[k][item[0]]= sim\n",
    "    return sort_dictionary(ar_train_sim_dic_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d43a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ar_img_files = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\train_mapped_face', eval_ar_name_list, tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e199bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/eval_face_similarity.tsv\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ar_img_files = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\crawl_train_face_mapped', eval_ar_name_list, crawl_tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/eval_face_similarity_crawl.tsv\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb5ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_face_matching = cal_face_matching_similarity(r\"result/eval_face_similarity.tsv\")\n",
    "eval_face_matching_crawl = cal_face_matching_similarity(r\"result/eval_face_similarity_crawl.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d012f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_MR(eval_face_matching):\n",
    "    count=0\n",
    "    for key, value in eval_face_matching.items():\n",
    "        first_tuple_elements=[]\n",
    "        for a_tuple in value:\n",
    "            first_tuple_elements.append(a_tuple)\n",
    "        if key in first_tuple_elements[0:100]:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb6dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21052631578947367\n",
      "0.0067085953878406705\n"
     ]
    }
   ],
   "source": [
    "print(cal_MR(eval_face_matching)/len(eval_face_matching))\n",
    "print(cal_MR(eval_face_matching)/2385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27fe665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10404624277456648\n",
      "0.007547169811320755\n"
     ]
    }
   ],
   "source": [
    "print(cal_MR(eval_face_matching_crawl)/len(eval_face_matching_crawl))\n",
    "print(cal_MR(eval_face_matching_crawl)/2385)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b763433",
   "metadata": {},
   "source": [
    "### Pediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_img_files = get_face_similarity(r'processed_data\\img\\test_face_candidate',\\\n",
    "                                   r'processed_data\\img\\train_eval_mapped_face', test_ar_name_list, tr_eval_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9461cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/test_train_img_similarity.txt\",eval_ar_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_img_files_crawl = get_face_similarity(r'processed_data\\img\\eval_face_candidate',\\\n",
    "                                   r'processed_data\\img\\train_eval_mapped_face', test_ar_name_list, tr_eval_crawl_tr_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_face_matching_similarity(r\"result/test_crawl_img_similarity.txt\",eval_ar_img_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
